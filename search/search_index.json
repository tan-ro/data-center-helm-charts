{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Atlassian Data Center Helm Charts \u00b6 This project contains Helm charts for installing Atlassian's Jira Data Center , Confluence Data Center , and Bitbucket Data Center on Kubernetes. Use the charts to install and operate Data Center products within a Kubernetes cluster of your choice. It can be a managed environment, such as Amazon EKS , Azure Kubernetes Service , Google Kubernetes Engine , or a custom on-premise system. Support disclaimer \u00b6 Warning These Helm charts are in Beta phase and unsupported , with the goal of introducing official support once they have been stabilized. Also, we don\u2019t officially support the functionality described in the examples or the documented platforms . You should use them for reference only. Architecture \u00b6 The diagram below provides a high level overview of what a typical deployment might look like when using the Atlassian Data Center Helm charts: Installing the Helm charts \u00b6 Prerequisites and setup - everything you need to do before installing the Helm charts Installation - the steps to install the Helm charts Migration - what you have to do if you're migrating an exisitng deployment to Kubernetes Additional content \u00b6 Operation - how to upgrade applications, scale your cluster, and update resources Configuration - a deep dive into the configuration parameters Platforms support - how to allow support for different platforms Examples - various configuration examples Feedback \u00b6 If you find any issue, please raise a ticket . If you have general feedback or question regarding the charts, please use Atlassian Community Kubernetes space . Contributions \u00b6 Contributions are welcome. Find out how to contribute . License \u00b6 Apache 2.0 licensed, see license file .","title":"Introduction"},{"location":"#atlassian-data-center-helm-charts","text":"This project contains Helm charts for installing Atlassian's Jira Data Center , Confluence Data Center , and Bitbucket Data Center on Kubernetes. Use the charts to install and operate Data Center products within a Kubernetes cluster of your choice. It can be a managed environment, such as Amazon EKS , Azure Kubernetes Service , Google Kubernetes Engine , or a custom on-premise system.","title":"Atlassian Data Center Helm Charts"},{"location":"#support-disclaimer","text":"Warning These Helm charts are in Beta phase and unsupported , with the goal of introducing official support once they have been stabilized. Also, we don\u2019t officially support the functionality described in the examples or the documented platforms . You should use them for reference only.","title":"Support disclaimer"},{"location":"#architecture","text":"The diagram below provides a high level overview of what a typical deployment might look like when using the Atlassian Data Center Helm charts:","title":"Architecture"},{"location":"#installing-the-helm-charts","text":"Prerequisites and setup - everything you need to do before installing the Helm charts Installation - the steps to install the Helm charts Migration - what you have to do if you're migrating an exisitng deployment to Kubernetes","title":"Installing the Helm charts"},{"location":"#additional-content","text":"Operation - how to upgrade applications, scale your cluster, and update resources Configuration - a deep dive into the configuration parameters Platforms support - how to allow support for different platforms Examples - various configuration examples","title":"Additional content"},{"location":"#feedback","text":"If you find any issue, please raise a ticket . If you have general feedback or question regarding the charts, please use Atlassian Community Kubernetes space .","title":"Feedback"},{"location":"#contributions","text":"Contributions are welcome. Find out how to contribute .","title":"Contributions"},{"location":"#license","text":"Apache 2.0 licensed, see license file .","title":"License"},{"location":"examples/EXAMPLES/","text":"Examples \u00b6 Support disclaimer Use the examples we provide as reference only, we don\u2019t offer official support for them. Kubernetes clusters \u00b6 See examples of provisioning Kubernetes clusters on cloud-based providers: Amazon EKS Google GKE Azure AKS Ingress \u00b6 See an example of provisioning an NGINX Ingress controller Database \u00b6 See an example of creating an Amazon RDS database instance Storage \u00b6 See examples of local and shared storage: AWS EFS \u00b6 Local storage - utilizing AWS EBS-backed volumes Shared storage - utilizing AWS EFS-backed filesystem NFS \u00b6 Implementation of an NFS Server for Bitbucket Elasticsearch \u00b6 See our recommendations for Bitbucket Elasticsearch Logging \u00b6 See an example of how to deploy an EFK stack to Kubernetes SSH \u00b6 See an example of SSH service in Bitbucket on Kubernetes","title":"Examples"},{"location":"examples/EXAMPLES/#examples","text":"Support disclaimer Use the examples we provide as reference only, we don\u2019t offer official support for them.","title":"Examples"},{"location":"examples/EXAMPLES/#kubernetes-clusters","text":"See examples of provisioning Kubernetes clusters on cloud-based providers: Amazon EKS Google GKE Azure AKS","title":"Kubernetes clusters"},{"location":"examples/EXAMPLES/#ingress","text":"See an example of provisioning an NGINX Ingress controller","title":"Ingress"},{"location":"examples/EXAMPLES/#database","text":"See an example of creating an Amazon RDS database instance","title":"Database"},{"location":"examples/EXAMPLES/#storage","text":"See examples of local and shared storage:","title":"Storage"},{"location":"examples/EXAMPLES/#aws-efs","text":"Local storage - utilizing AWS EBS-backed volumes Shared storage - utilizing AWS EFS-backed filesystem","title":"AWS EFS"},{"location":"examples/EXAMPLES/#nfs","text":"Implementation of an NFS Server for Bitbucket","title":"NFS"},{"location":"examples/EXAMPLES/#elasticsearch","text":"See our recommendations for Bitbucket Elasticsearch","title":"Elasticsearch"},{"location":"examples/EXAMPLES/#logging","text":"See an example of how to deploy an EFK stack to Kubernetes","title":"Logging"},{"location":"examples/EXAMPLES/#ssh","text":"See an example of SSH service in Bitbucket on Kubernetes","title":"SSH"},{"location":"examples/cluster/AKS_SETUP/","text":"Preparing an AKS cluster \u00b6 This example provides instructions for creating a Kubernetes cluster using Azure AKS . Prerequisites \u00b6 We recommend installing and configuring the Azure Cloud Shell , allowing for CLI interaction with the AKS cluster. Manual creation \u00b6 Follow the Azure Kubernetes Service Quickstart for details on creating an AKS cluster. Having established a cluster continue with provisioning the prerequisite infrastructure .","title":"Preparing an AKS cluster"},{"location":"examples/cluster/AKS_SETUP/#preparing-an-aks-cluster","text":"This example provides instructions for creating a Kubernetes cluster using Azure AKS .","title":"Preparing an AKS cluster"},{"location":"examples/cluster/AKS_SETUP/#prerequisites","text":"We recommend installing and configuring the Azure Cloud Shell , allowing for CLI interaction with the AKS cluster.","title":"Prerequisites"},{"location":"examples/cluster/AKS_SETUP/#manual-creation","text":"Follow the Azure Kubernetes Service Quickstart for details on creating an AKS cluster. Having established a cluster continue with provisioning the prerequisite infrastructure .","title":"Manual creation"},{"location":"examples/cluster/CLOUD_PROVIDERS/","text":"Provisioning Kubernetes clusters on cloud-based providers \u00b6 Here are installation and configuration instructions for cloud providers: Amazon EKS Google GKE Azure AKS","title":"Provisioning Kubernetes clusters on cloud-based providers"},{"location":"examples/cluster/CLOUD_PROVIDERS/#provisioning-kubernetes-clusters-on-cloud-based-providers","text":"Here are installation and configuration instructions for cloud providers: Amazon EKS Google GKE Azure AKS","title":"Provisioning Kubernetes clusters on cloud-based providers"},{"location":"examples/cluster/EKS_SETUP/","text":"Preparing an EKS cluster \u00b6 This example provides instructions for creating a Kubernetes cluster using Amazon EKS . Prerequisites \u00b6 We recommend installing and configuring eksctl , allowing for CLI interaction with the EKS cluster. Manual creation \u00b6 Follow the Getting started with Amazon EKS for details on creating an EKS cluster. Or, using the ClusterConfig below as an example, deploy a K8s cluster with eksctl in ~20 minutes: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : atlassian-cluster region : ap-southeast-2 managedNodeGroups : - name : jira instanceType : m5.large desiredCapacity : 2 ssh : # enable SSH using SSM enableSsm : true Cluster considerations It's always a good idea to consider the following points before creating the cluster: Geographical region - where will the cluster reside. EC2 instance type - the instance type to be used for the nodes that make up the cluster. Number of nodes - guidance on the resource dimensions that should be used for these nodes can be found in Requests and limits . Adding the config above to a file named config.yaml provision the cluster: eksctl create cluster -f config.yaml Having established a cluster, continue with provisioning the prerequisite infrastructure .","title":"Preparing an EKS cluster"},{"location":"examples/cluster/EKS_SETUP/#preparing-an-eks-cluster","text":"This example provides instructions for creating a Kubernetes cluster using Amazon EKS .","title":"Preparing an EKS cluster"},{"location":"examples/cluster/EKS_SETUP/#prerequisites","text":"We recommend installing and configuring eksctl , allowing for CLI interaction with the EKS cluster.","title":"Prerequisites"},{"location":"examples/cluster/EKS_SETUP/#manual-creation","text":"Follow the Getting started with Amazon EKS for details on creating an EKS cluster. Or, using the ClusterConfig below as an example, deploy a K8s cluster with eksctl in ~20 minutes: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : atlassian-cluster region : ap-southeast-2 managedNodeGroups : - name : jira instanceType : m5.large desiredCapacity : 2 ssh : # enable SSH using SSM enableSsm : true Cluster considerations It's always a good idea to consider the following points before creating the cluster: Geographical region - where will the cluster reside. EC2 instance type - the instance type to be used for the nodes that make up the cluster. Number of nodes - guidance on the resource dimensions that should be used for these nodes can be found in Requests and limits . Adding the config above to a file named config.yaml provision the cluster: eksctl create cluster -f config.yaml Having established a cluster, continue with provisioning the prerequisite infrastructure .","title":"Manual creation"},{"location":"examples/cluster/GKE_SETUP/","text":"Preparing an GKE cluster \u00b6 This example provides instructions for creating a Kubernetes cluster using Google GKE . Prerequisites \u00b6 We recommend installing and configuring Google Cloud SDK , allowing for CLI interaction with an GKE cluster. Manual creation \u00b6 Follow the How-to guides for details on creating an GKE cluster. Having established a cluster continue with provisioning the prerequisite infrastructure .","title":"Preparing an GKE cluster"},{"location":"examples/cluster/GKE_SETUP/#preparing-an-gke-cluster","text":"This example provides instructions for creating a Kubernetes cluster using Google GKE .","title":"Preparing an GKE cluster"},{"location":"examples/cluster/GKE_SETUP/#prerequisites","text":"We recommend installing and configuring Google Cloud SDK , allowing for CLI interaction with an GKE cluster.","title":"Prerequisites"},{"location":"examples/cluster/GKE_SETUP/#manual-creation","text":"Follow the How-to guides for details on creating an GKE cluster. Having established a cluster continue with provisioning the prerequisite infrastructure .","title":"Manual creation"},{"location":"examples/database/AMAZON_RDS/","text":"Creating an RDS database instance \u00b6 This example provides instructions for creating an Amazon RDS DB instance . Prerequisites \u00b6 An AWS account , IAM user , VPC (or default VPC ) and security group are required before an RDS DB instance can be created. See Setting up for Amazon RDS for further instructions. Database creation \u00b6 There are two steps for creating the database: 1. Initialize database server \u00b6 For details on standing up an RDS DB server follow the guide: Creating an Amazon RDS DB instance . 2. Create database \u00b6 For details on creating the database user and database itself follow the appropriate guide below: Jira Create database for Jira Confluence Create database for Confluence Bitbucket Create database for Bitbucket Crowd Create database for Crowd Once you create a database continue with provisioning the prerequisite infrastructure .","title":"Creating an RDS database instance"},{"location":"examples/database/AMAZON_RDS/#creating-an-rds-database-instance","text":"This example provides instructions for creating an Amazon RDS DB instance .","title":"Creating an RDS database instance"},{"location":"examples/database/AMAZON_RDS/#prerequisites","text":"An AWS account , IAM user , VPC (or default VPC ) and security group are required before an RDS DB instance can be created. See Setting up for Amazon RDS for further instructions.","title":"Prerequisites"},{"location":"examples/database/AMAZON_RDS/#database-creation","text":"There are two steps for creating the database:","title":"Database creation"},{"location":"examples/database/AMAZON_RDS/#1-initialize-database-server","text":"For details on standing up an RDS DB server follow the guide: Creating an Amazon RDS DB instance .","title":"1. Initialize database server"},{"location":"examples/database/AMAZON_RDS/#2-create-database","text":"For details on creating the database user and database itself follow the appropriate guide below: Jira Create database for Jira Confluence Create database for Confluence Bitbucket Create database for Bitbucket Crowd Create database for Crowd Once you create a database continue with provisioning the prerequisite infrastructure .","title":"2. Create database"},{"location":"examples/database/CLOUD_PROVIDERS/","text":"Provisioning databases on cloud-based providers \u00b6 Database deployment and configuration instructions for cloud providers can be found below: Amazon RDS Your selected database engine type must be supported by the Data Center product you wish to install: Jira Jira supported databases Confluence Confluence supported databases Bitbucket Bitbucket supported databases Crowd Crowd supported databases","title":"Provisioning databases on cloud-based providers"},{"location":"examples/database/CLOUD_PROVIDERS/#provisioning-databases-on-cloud-based-providers","text":"Database deployment and configuration instructions for cloud providers can be found below: Amazon RDS Your selected database engine type must be supported by the Data Center product you wish to install: Jira Jira supported databases Confluence Confluence supported databases Bitbucket Bitbucket supported databases Crowd Crowd supported databases","title":"Provisioning databases on cloud-based providers"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/","text":"Bitbucket Elasticsearch recommendations \u00b6 While Bitbucket has its own internal Elasticsearch instance, we highly recommend you use an external Elasticsearch installation, either within the Kubernetes cluster or, if available, an instance managed by your hosting provider. Installing and configuring Elasticsearch in your Kubernetes cluster \u00b6 Installing Elasticsearch into your Kubernetes cluster \u00b6 Choose a version of Elasticsearch that is supported by the version of Bitbucket you are installing . For Bitbucket 7.14 the latest supported Elasticsearch version is 7.9.3, so we will target that. There are official Helm charts for Elasticsearch 7.9.3 . Following the documentation there add the Elasticsearch Helm charts repository, then install it: helm repo add elastic https://helm.elastic.co helm install elasticsearch --version 7.9.3 elastic/elasticsearch Configuring your Bitbucket deployment \u00b6 To enable using the installed Elasticsearch service you need to to configure the service URL under bitbucket: in the values.yaml file: bitbucket : elasticSearch : baseUrl : http://elasticsearch:9200 This will also have the effect of disabling Bitbucket\u2019s internal Elasticsearch instance. If you have configured authentication in the deployed Elasticsearch you will also need to provide the details in a Kubernetes secret and configure that in the values.yaml file: credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets . Configuring Amazon Elasticsearch Service with Bitbucket on Kubernetes \u00b6 Creating an Amazon Elasticsearch Service domain with a master user \u00b6 The Elasticsearch instance (\u201cdomain\u201d) can be created via the AWS CLI or the web console; for this example we will use the web console and a master user: In the EKS console navigate to Your Cluster \u2192 Networking and note the VPC ID. In the Elasticsearch console create a new domain: Select a production deployment. Select Elasticsearch version 7.9. In the next screen configure the AZs and nodes as appropriate for your expected workload. On the Access and security page: Select the same VPC as the EKS cluster, as noted in step 1. Select appropriate subnets for each AZ; private subnets are fine. Select appropriate security groups that will grant node/pod access. Tick Fine\u2013grained access control : Select Create master user and add a username and a strong password. Configure tags, etc. as appropriate for your organisation. Once the Elasticsearch domain has finished creating, make a note of the VPC Endpoint , which will be an HTTPS URL. Configuring your Bitbucket deployment \u00b6 To use the managed Elasticsearch service, first create a Kubernetes secret using the username and password from step 4 above. Then configure the service URL under bitbucket: in the values.yaml file, substituting the values below from the above steps where appropriate: bitbucket : elasticSearch : baseUrl : <VPC Endpoint> credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets .","title":"Bitbucket Elasticsearch recommendations"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#bitbucket-elasticsearch-recommendations","text":"While Bitbucket has its own internal Elasticsearch instance, we highly recommend you use an external Elasticsearch installation, either within the Kubernetes cluster or, if available, an instance managed by your hosting provider.","title":"Bitbucket Elasticsearch recommendations"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#installing-and-configuring-elasticsearch-in-your-kubernetes-cluster","text":"","title":"Installing and configuring Elasticsearch in your Kubernetes cluster"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#installing-elasticsearch-into-your-kubernetes-cluster","text":"Choose a version of Elasticsearch that is supported by the version of Bitbucket you are installing . For Bitbucket 7.14 the latest supported Elasticsearch version is 7.9.3, so we will target that. There are official Helm charts for Elasticsearch 7.9.3 . Following the documentation there add the Elasticsearch Helm charts repository, then install it: helm repo add elastic https://helm.elastic.co helm install elasticsearch --version 7.9.3 elastic/elasticsearch","title":"Installing Elasticsearch into your Kubernetes cluster"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment","text":"To enable using the installed Elasticsearch service you need to to configure the service URL under bitbucket: in the values.yaml file: bitbucket : elasticSearch : baseUrl : http://elasticsearch:9200 This will also have the effect of disabling Bitbucket\u2019s internal Elasticsearch instance. If you have configured authentication in the deployed Elasticsearch you will also need to provide the details in a Kubernetes secret and configure that in the values.yaml file: credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets .","title":"Configuring your Bitbucket deployment"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#configuring-amazon-elasticsearch-service-with-bitbucket-on-kubernetes","text":"","title":"Configuring Amazon Elasticsearch Service with Bitbucket on Kubernetes"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#creating-an-amazon-elasticsearch-service-domain-with-a-master-user","text":"The Elasticsearch instance (\u201cdomain\u201d) can be created via the AWS CLI or the web console; for this example we will use the web console and a master user: In the EKS console navigate to Your Cluster \u2192 Networking and note the VPC ID. In the Elasticsearch console create a new domain: Select a production deployment. Select Elasticsearch version 7.9. In the next screen configure the AZs and nodes as appropriate for your expected workload. On the Access and security page: Select the same VPC as the EKS cluster, as noted in step 1. Select appropriate subnets for each AZ; private subnets are fine. Select appropriate security groups that will grant node/pod access. Tick Fine\u2013grained access control : Select Create master user and add a username and a strong password. Configure tags, etc. as appropriate for your organisation. Once the Elasticsearch domain has finished creating, make a note of the VPC Endpoint , which will be an HTTPS URL.","title":"Creating an Amazon Elasticsearch Service domain with a master user"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment_1","text":"To use the managed Elasticsearch service, first create a Kubernetes secret using the username and password from step 4 above. Then configure the service URL under bitbucket: in the values.yaml file, substituting the values below from the above steps where appropriate: bitbucket : elasticSearch : baseUrl : <VPC Endpoint> credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets .","title":"Configuring your Bitbucket deployment"},{"location":"examples/ingress/CONTROLLERS/","text":"Provisioning an Ingress controller \u00b6 In order for the provided Ingress resource to work, your Kubernetes cluster must have an ingress controller running. The Atlassian Helm charts have been tested with the NGINX Ingress Controller , however alternatives can also be used . Here is an example of how these controllers can be installed and configured for use with the Atlassian Helm charts: NGINX Ingress Controller","title":"Provisioning an Ingress controller"},{"location":"examples/ingress/CONTROLLERS/#provisioning-an-ingress-controller","text":"In order for the provided Ingress resource to work, your Kubernetes cluster must have an ingress controller running. The Atlassian Helm charts have been tested with the NGINX Ingress Controller , however alternatives can also be used . Here is an example of how these controllers can be installed and configured for use with the Atlassian Helm charts: NGINX Ingress Controller","title":"Provisioning an Ingress controller"},{"location":"examples/ingress/INGRESS_NGINX/","text":"NGINX Ingress Controller - with TLS termination \u00b6 NGINX ingress controller with automatic TLS certificate management using cert-manager and certificates from Let's Encrypt . NOTE: These instructions are for reference purposes only. They should be used for development and testing purposes only! Official instructions for deploying and configuring the controller can be found here . These instructions are composed of 3 high-level parts: Controller installation and configuration Certificate manager installation and configuration Ingress resource configuration Controller installation and configuration \u00b6 We recommend installing the controller using its official Helm Charts . You can also use the instructions below. 1. Add controller repo \u00b6 Add the ingress-nginx Helm repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Update the repo: helm repo update 2. Install controller \u00b6 Create a new namespace for the Ingress controller: kubectl create namespace ingress Install the controller using Helm: helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress 3. DNS setup \u00b6 Manually provision a new DNS record via your cloud provider or using external-dns . Once created, associate the DNS record with the auto provisioned Load Balancer that was created in Step 2. above . To do this first identify the name of the auto provisioned LB, this can be done by examining the deployed ingress services i.e. kubectl get service -n ingress | grep ingress-nginx the output of this command should look something like... ingress-nginx-controller LoadBalancer 10 .100.22.16 b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com 80 :32615/TCP,443:31787/TCP 76m ingress-nginx-controller-admission ClusterIP 10 .100.5.36 <none> 443 /TCP 76m Take note of the LoadBalancer and using it as a value update the DNS record so that traffic is routed to it. NOTE: It can take a few minutes for the DNS to resolve these changes. Certificate manager installation and configuration \u00b6 K8s certificate management is handled using cert-manager . 1. Install cert-manager \u00b6 Add the cert manager repo helm repo add jetstack https://charts.jetstack.io Update repos helm repo update Install the cert-manager using Helm helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.3.1 \\ --set installCRDs = true Confirm the cert-manager is appropriately installed: kubectl get pods --namespace cert-manager 2. Create certificate issuer \u00b6 Using the yaml spec below create and apply the certificate Issuer resource Ensure that the certificate issuer is installed in the same namespace that the Atlassian product will be deployed to. apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-prod namespace : <product_deployment_namespace> spec : acme : # The ACME server URL server : https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : <user_email> # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-prod # Enable the HTTP-01 challenge provider solvers : - http01 : ingress : class : nginx Install the Issuer resource kubectl apply -f issuer.yaml Ingress resource configuration \u00b6 Now that the Ingress controller and certificate manager are setup the Ingress resource can be configured accordingly by updating the values.yaml . 1. Ingress resource config \u00b6 For TLS cert auto-provisioning and TLS termination update the ingress stanza within the products values.yaml : ingress : create : true nginx : true maxBodySize : 250m host : <dns_record> path : \"/\" annotations : cert-manager.io/issuer : \"letsencrypt-prod\" # Using https://letsencrypt.org/ https : true tlsSecretName : tls-certificate Bitbucket SSH configuration \u00b6 NOTE: Bitbucket requires additional Ingress config to allow for SSH access. See NGINX Ingress controller config for SSH connections for details. Having created the Ingress controller continue with provisioning the prerequisite infrastructure .","title":"NGINX Ingress Controller - with TLS termination"},{"location":"examples/ingress/INGRESS_NGINX/#nginx-ingress-controller-with-tls-termination","text":"NGINX ingress controller with automatic TLS certificate management using cert-manager and certificates from Let's Encrypt . NOTE: These instructions are for reference purposes only. They should be used for development and testing purposes only! Official instructions for deploying and configuring the controller can be found here . These instructions are composed of 3 high-level parts: Controller installation and configuration Certificate manager installation and configuration Ingress resource configuration","title":"NGINX Ingress Controller - with TLS termination"},{"location":"examples/ingress/INGRESS_NGINX/#controller-installation-and-configuration","text":"We recommend installing the controller using its official Helm Charts . You can also use the instructions below.","title":"Controller installation and configuration"},{"location":"examples/ingress/INGRESS_NGINX/#1-add-controller-repo","text":"Add the ingress-nginx Helm repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Update the repo: helm repo update","title":"1. Add controller repo"},{"location":"examples/ingress/INGRESS_NGINX/#2-install-controller","text":"Create a new namespace for the Ingress controller: kubectl create namespace ingress Install the controller using Helm: helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress","title":"2. Install controller"},{"location":"examples/ingress/INGRESS_NGINX/#3-dns-setup","text":"Manually provision a new DNS record via your cloud provider or using external-dns . Once created, associate the DNS record with the auto provisioned Load Balancer that was created in Step 2. above . To do this first identify the name of the auto provisioned LB, this can be done by examining the deployed ingress services i.e. kubectl get service -n ingress | grep ingress-nginx the output of this command should look something like... ingress-nginx-controller LoadBalancer 10 .100.22.16 b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com 80 :32615/TCP,443:31787/TCP 76m ingress-nginx-controller-admission ClusterIP 10 .100.5.36 <none> 443 /TCP 76m Take note of the LoadBalancer and using it as a value update the DNS record so that traffic is routed to it. NOTE: It can take a few minutes for the DNS to resolve these changes.","title":"3. DNS setup"},{"location":"examples/ingress/INGRESS_NGINX/#certificate-manager-installation-and-configuration","text":"K8s certificate management is handled using cert-manager .","title":"Certificate manager installation and configuration"},{"location":"examples/ingress/INGRESS_NGINX/#1-install-cert-manager","text":"Add the cert manager repo helm repo add jetstack https://charts.jetstack.io Update repos helm repo update Install the cert-manager using Helm helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.3.1 \\ --set installCRDs = true Confirm the cert-manager is appropriately installed: kubectl get pods --namespace cert-manager","title":"1. Install cert-manager"},{"location":"examples/ingress/INGRESS_NGINX/#2-create-certificate-issuer","text":"Using the yaml spec below create and apply the certificate Issuer resource Ensure that the certificate issuer is installed in the same namespace that the Atlassian product will be deployed to. apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-prod namespace : <product_deployment_namespace> spec : acme : # The ACME server URL server : https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : <user_email> # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-prod # Enable the HTTP-01 challenge provider solvers : - http01 : ingress : class : nginx Install the Issuer resource kubectl apply -f issuer.yaml","title":"2. Create certificate issuer"},{"location":"examples/ingress/INGRESS_NGINX/#ingress-resource-configuration","text":"Now that the Ingress controller and certificate manager are setup the Ingress resource can be configured accordingly by updating the values.yaml .","title":"Ingress resource configuration"},{"location":"examples/ingress/INGRESS_NGINX/#1-ingress-resource-config","text":"For TLS cert auto-provisioning and TLS termination update the ingress stanza within the products values.yaml : ingress : create : true nginx : true maxBodySize : 250m host : <dns_record> path : \"/\" annotations : cert-manager.io/issuer : \"letsencrypt-prod\" # Using https://letsencrypt.org/ https : true tlsSecretName : tls-certificate","title":"1. Ingress resource config"},{"location":"examples/ingress/INGRESS_NGINX/#bitbucket-ssh-configuration","text":"NOTE: Bitbucket requires additional Ingress config to allow for SSH access. See NGINX Ingress controller config for SSH connections for details. Having created the Ingress controller continue with provisioning the prerequisite infrastructure .","title":"Bitbucket SSH configuration"},{"location":"examples/logging/efk/EFK/","text":"Logging in a Kubernetes environment \u00b6 Warning This functionality is not officially supported. This document explains how to enable aggregated logging in your Kubernetes cluster. There are many ways to do this and this document showcases only a few of the options. EFK stack \u00b6 A common Kubernetes logging pattern is the combination of Elasticsearch , Fluentd , and Kibana , known as EFK Stack . Fluentd is an open-source and multi-platform log processor that collects data/logs from different sources, aggregates, and forwards them to multiple destinations. It is fully compatible with Docker and Kubernetes environments. Elasticsearch is a distributed open search and analytics engine for all types of data. Kibana is an open-source front-end application that sits on top of Elasticsearch, providing search and data visualization capabilities for data indexed in Elasticsearch. There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster. EFK using local Elasticsearch \u00b6 This solution deploys all of the EFK stack inside the Kubernetes cluster. By setting fluentd.enabled value to true , Helm installs Fluentd on each of application pods. This means that after deployment all the product pods run Fluentd, which collects all the log files and sends them to the Fluentd aggregator container. To complete the EFK stack you need to install an Elasticsearch cluster and Kibana, and successfully forward the aggregated datalog to Elasticsearch using Fluentd, which is already installed. Follow these steps to install Elasticsearch: $ helm repo add elastic https://helm.elastic.co \"elastic\" has been added to your repositories $ helm install elasticsearch elastic/elasticsearch ... Wait until all the nodes start and the status changes to Running : $ kubectl get pods --namespace = dcd -l app = elasticsearch-master NAME READY STATUS RESTARTS AGE elasticsearch-master-0 1 /1 Running 0 100m elasticsearch-master-1 1 /1 Running 0 100m elasticsearch-master-2 1 /1 Running 0 100m $ $ kubectl port-forward svc/elasticsearch-master 9200 Make sure Elasticsearch cluster is working as expected: $ curl localhost:9200 { \"name\" : \"elasticsearch-master-0\" , \"cluster_name\" : \"elasticsearch\" , \"cluster_uuid\" : \"uNdYC-2nSdWVdzPCw9P7jQ\" , \"version\" : { \"number\" : \"7.12.0\" , \"build_flavor\" : \"default\" , \"build_type\" : \"docker\" , \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\" , \"build_date\" : \"2021-03-18T06:17:15.410153305Z\" , \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" } , \"tagline\" : \"You Know, for Search\" } Now enable fluentd and set the hostname for Elasticsearch in values.yaml as follows: fluentd : enabled : true elasticsearch : hostname : elasticsearch-master Fluentd tries to parse and send the data to Elasticsearch, but since it's not installed yet the data is lost. At this point you have logged data in the installed Elasticsearch, and you should install Kibana to complete the EFK stack deployment: $ helm install kibana elastic/kibana NAME: kibana LAST DEPLOYED: Wed Apr 14 12 :52:21 2021 NAMESPACE: dcd STATUS: DEPLOYED Make sure kibana is deployed and then setup port forwarding for kibana: $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE helm-operator 1 /1 1 1 23m ingress-nginx-release-controller 1 /1 1 1 22m kibana-kibana 1 /1 1 1 25m $ kubectl port-forward deployment/kibana-kibana 5601 You can access Kibana via the browser: http://localhost:5601. To visualise the logs you need to create an index pattern and then look at the the data in the discovery part. To create the index pattern go to Management \u2192 Stack Management and then select Kibana \u2192 Index Patterns . Managed Elasticsearch in AWS \u00b6 In this solution Elasticsearch deploys as a managed service and lives outside of the Kubernetes cluster. For this purpose use Fluent Bit instead of Fluentd for local deployment of EFK. When a node inside an EKS cluster needs to call an AWS API, it needs to provide extended permissions. Amazon provides an image of Fluent Bit that supports AWS service accounts,and using this you no longer need to follow the traditional way. All you need is to have an IAM role for the AWS service account on an EKS cluster. So using this service account, an AWS permission can be provided to the containers in any pod that use that service account. The result is that the pods on that node can call AWS APIs. fluentbit is used to collect and aggregate the data inside the EKS cluster, which communicates with AWS Elasticsearch outside of the cluster. Your first step is to configure IAM roles for Service Accounts (IRSA) for fluentbit , to make sure you have an OIDC identity provider to use IAM roles for the service account in the cluster: $ eksctl utils associate-iam-oidc-provider \\ --cluster dcd-ap-southeast-2 \\ --approve Then create an IAM policy to limit the permissions to connect to the Elasticsearch cluster. Before this, you need to set the following environment variables: * KUBE_NAMESPACE : The namespace for kubernetes cluster * ES_DOMAIN_NAME : Elasticsearch domain name * ES_VERSION : Elasticsearch version * ES_USER : Elasticsearch username * ES_PASSWORD : Elasticsearch password (eg. export ES_PASSWORD=\"$(openssl rand -base64 8)_Ek1$\" ) * ACCOUNT_ID : AWS Account ID * AWS_REGION : AWS region code $ mkdir ~/environment/logging $ cat <<EoF > ~/environment/logging/fluent-bit-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"es:ESHttp*\" ], \"Resource\": \"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}\", \"Effect\": \"Allow\" } ] } EoF $ aws iam create-policy \\ --policy-name fluent-bit-policy \\ --policy-document file://~/environment/logging/fluent-bit-policy.json Next, create an IAM role for the service account: eksctl create iamserviceaccount \\ --name fluent-bit \\ --namespace dcd \\ --cluster dcd-ap-southeast-2 \\ --attach-policy-arn \"arn:aws:iam:: ${ ACCOUNT_ID } :policy/fluent-bit-policy\" \\ --approve \\ --override-existing-serviceaccounts To confirm that the service account with an Amazon Resource Name (ARN) of the IAM role is annotated: $ kubectl describe serviceaccount fluent-bit Name: fluent-bit Namespace: dcd Labels: <none> Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-9B18FAAFE02F6 Image pull secrets: <none> Mountable secrets: fluent-bit-token-pgpss Tokens: fluent-bit-token-pgpss Events: <none> Provision an Elasticsearch cluster: Provision a public Elasticsearch cluster with Fine-Grained Access Control enabled and a built-in user database: $ cat <<EOF> ~/environment/logging/elasticsearch_domain.json { \"DomainName\": ${ES_DOMAIN_NAME}, \"ElasticsearchVersion\": ${ES_VERSION}, \"ElasticsearchClusterConfig\": { \"InstanceType\": \"r5.large.elasticsearch\", \"InstanceCount\": 1, \"DedicatedMasterEnabled\": false, \"ZoneAwarenessEnabled\": false, \"WarmEnabled\": false }, \"EBSOptions\": { \"EBSEnabled\": true, \"VolumeType\": \"gp2\", \"VolumeSize\": 100 }, \"AccessPolicies\": \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"AWS\\\":\\\"*\\\"},\\\"Action\\\":\\\"es:ESHttp*\\\",\\\"Resource\\\":\\\"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}/*\\\"}]}\", \"SnapshotOptions\": {}, \"CognitoOptions\": { \"Enabled\": false }, \"EncryptionAtRestOptions\": { \"Enabled\": true }, \"NodeToNodeEncryptionOptions\": { \"Enabled\": true }, \"DomainEndpointOptions\": { \"EnforceHTTPS\": true, \"TLSSecurityPolicy\": \"Policy-Min-TLS-1-0-2019-07\" }, \"AdvancedSecurityOptions\": { \"Enabled\": true, \"InternalUserDatabaseEnabled\": true, \"MasterUserOptions\": { \"MasterUserName\": ${ES_USER}, \"MasterUserPassword\": ${ES_PASSWORD} } } } EOF $ aws es create-elasticsearch-domain \\ --cli-input-json file://~/environment/logging/es_domain.json It takes a while for Elasticsearch clusters to change to an active state. Check the AWS Console to see the status of the cluster, and continue to the next step when the cluster is ready. Configure Elasticsearch access: At this point you need to map roles to users in order to set fine-grained access control, because without this mapping all the requests to the cluster will result in permission errors. You should add the Fluent Bit ARN as a backend role to the all-access role, which uses the Elasticsearch APIs. To find the Fluent Bit ARN run the following command and export the value of ARN Role into the FLUENTBIT_ROLE environment variable: $ eksctl get iamserviceaccount --cluster dcd-ap-southeast-2 [ \u2139 ] eksctl version 0 .37.0 [ \u2139 ] using region ap-southeast-2 NAMESPACE NAME ROLE ARN kube-system cluster-autoscaler arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E $ export FLUENTBIT_ROLE = arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E Retrieve the Elasticsearch endpoint and update the internal database: $ export ES_ENDPOINT = $( aws es describe-elasticsearch-domain --domain-name ngh-search-domain --output text --query \"DomainStatus.Endpoint\" ) $ curl -sS -u \" ${ ES_DOMAIN_USER } : ${ ES_DOMAIN_PASSWORD } \" \\ -X PATCH \\ https:// ${ ES_ENDPOINT } /_opendistro/_security/api/rolesmapping/all_access?pretty \\ -H 'Content-Type: application/json' \\ -d ' [ { \"op\": \"add\", \"path\": \"/backend_roles\", \"value\": [\"' ${ FLUENTBIT_ROLE } '\"] } ] ' Finally, it is time to deploy Fluent Bit DaemonSet: $ kubectl apply -f src/main/logging/fluentbit.yaml After a few minutes all pods should be up and in running status. This is the end of the, you can open Kibana to visualise the logs. The endpoint for Kibana can be found in the Elasticsearch output tab in the AWS console, or you can run the following command: $ echo \"Kibana URL: https:// ${ ES_ENDPOINT } /_plugin/kibana/\" Kibana URL: https://search-domain-uehlb3kxledxykchwexee.ap-southeast-2.es.amazonaws.com/_plugin/kibana/ The user and password for Kibana are the same as the master user credential that is set in Elasticsearch in the provisioning stage. Open Kibana in a browser and after login, create an index pattern and see the report in the Discover page.","title":"Logging in a Kubernetes environment"},{"location":"examples/logging/efk/EFK/#logging-in-a-kubernetes-environment","text":"Warning This functionality is not officially supported. This document explains how to enable aggregated logging in your Kubernetes cluster. There are many ways to do this and this document showcases only a few of the options.","title":"Logging in a Kubernetes environment"},{"location":"examples/logging/efk/EFK/#efk-stack","text":"A common Kubernetes logging pattern is the combination of Elasticsearch , Fluentd , and Kibana , known as EFK Stack . Fluentd is an open-source and multi-platform log processor that collects data/logs from different sources, aggregates, and forwards them to multiple destinations. It is fully compatible with Docker and Kubernetes environments. Elasticsearch is a distributed open search and analytics engine for all types of data. Kibana is an open-source front-end application that sits on top of Elasticsearch, providing search and data visualization capabilities for data indexed in Elasticsearch. There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster.","title":"EFK stack"},{"location":"examples/logging/efk/EFK/#efk-using-local-elasticsearch","text":"This solution deploys all of the EFK stack inside the Kubernetes cluster. By setting fluentd.enabled value to true , Helm installs Fluentd on each of application pods. This means that after deployment all the product pods run Fluentd, which collects all the log files and sends them to the Fluentd aggregator container. To complete the EFK stack you need to install an Elasticsearch cluster and Kibana, and successfully forward the aggregated datalog to Elasticsearch using Fluentd, which is already installed. Follow these steps to install Elasticsearch: $ helm repo add elastic https://helm.elastic.co \"elastic\" has been added to your repositories $ helm install elasticsearch elastic/elasticsearch ... Wait until all the nodes start and the status changes to Running : $ kubectl get pods --namespace = dcd -l app = elasticsearch-master NAME READY STATUS RESTARTS AGE elasticsearch-master-0 1 /1 Running 0 100m elasticsearch-master-1 1 /1 Running 0 100m elasticsearch-master-2 1 /1 Running 0 100m $ $ kubectl port-forward svc/elasticsearch-master 9200 Make sure Elasticsearch cluster is working as expected: $ curl localhost:9200 { \"name\" : \"elasticsearch-master-0\" , \"cluster_name\" : \"elasticsearch\" , \"cluster_uuid\" : \"uNdYC-2nSdWVdzPCw9P7jQ\" , \"version\" : { \"number\" : \"7.12.0\" , \"build_flavor\" : \"default\" , \"build_type\" : \"docker\" , \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\" , \"build_date\" : \"2021-03-18T06:17:15.410153305Z\" , \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" } , \"tagline\" : \"You Know, for Search\" } Now enable fluentd and set the hostname for Elasticsearch in values.yaml as follows: fluentd : enabled : true elasticsearch : hostname : elasticsearch-master Fluentd tries to parse and send the data to Elasticsearch, but since it's not installed yet the data is lost. At this point you have logged data in the installed Elasticsearch, and you should install Kibana to complete the EFK stack deployment: $ helm install kibana elastic/kibana NAME: kibana LAST DEPLOYED: Wed Apr 14 12 :52:21 2021 NAMESPACE: dcd STATUS: DEPLOYED Make sure kibana is deployed and then setup port forwarding for kibana: $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE helm-operator 1 /1 1 1 23m ingress-nginx-release-controller 1 /1 1 1 22m kibana-kibana 1 /1 1 1 25m $ kubectl port-forward deployment/kibana-kibana 5601 You can access Kibana via the browser: http://localhost:5601. To visualise the logs you need to create an index pattern and then look at the the data in the discovery part. To create the index pattern go to Management \u2192 Stack Management and then select Kibana \u2192 Index Patterns .","title":"EFK using local Elasticsearch"},{"location":"examples/logging/efk/EFK/#managed-elasticsearch-in-aws","text":"In this solution Elasticsearch deploys as a managed service and lives outside of the Kubernetes cluster. For this purpose use Fluent Bit instead of Fluentd for local deployment of EFK. When a node inside an EKS cluster needs to call an AWS API, it needs to provide extended permissions. Amazon provides an image of Fluent Bit that supports AWS service accounts,and using this you no longer need to follow the traditional way. All you need is to have an IAM role for the AWS service account on an EKS cluster. So using this service account, an AWS permission can be provided to the containers in any pod that use that service account. The result is that the pods on that node can call AWS APIs. fluentbit is used to collect and aggregate the data inside the EKS cluster, which communicates with AWS Elasticsearch outside of the cluster. Your first step is to configure IAM roles for Service Accounts (IRSA) for fluentbit , to make sure you have an OIDC identity provider to use IAM roles for the service account in the cluster: $ eksctl utils associate-iam-oidc-provider \\ --cluster dcd-ap-southeast-2 \\ --approve Then create an IAM policy to limit the permissions to connect to the Elasticsearch cluster. Before this, you need to set the following environment variables: * KUBE_NAMESPACE : The namespace for kubernetes cluster * ES_DOMAIN_NAME : Elasticsearch domain name * ES_VERSION : Elasticsearch version * ES_USER : Elasticsearch username * ES_PASSWORD : Elasticsearch password (eg. export ES_PASSWORD=\"$(openssl rand -base64 8)_Ek1$\" ) * ACCOUNT_ID : AWS Account ID * AWS_REGION : AWS region code $ mkdir ~/environment/logging $ cat <<EoF > ~/environment/logging/fluent-bit-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"es:ESHttp*\" ], \"Resource\": \"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}\", \"Effect\": \"Allow\" } ] } EoF $ aws iam create-policy \\ --policy-name fluent-bit-policy \\ --policy-document file://~/environment/logging/fluent-bit-policy.json Next, create an IAM role for the service account: eksctl create iamserviceaccount \\ --name fluent-bit \\ --namespace dcd \\ --cluster dcd-ap-southeast-2 \\ --attach-policy-arn \"arn:aws:iam:: ${ ACCOUNT_ID } :policy/fluent-bit-policy\" \\ --approve \\ --override-existing-serviceaccounts To confirm that the service account with an Amazon Resource Name (ARN) of the IAM role is annotated: $ kubectl describe serviceaccount fluent-bit Name: fluent-bit Namespace: dcd Labels: <none> Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-9B18FAAFE02F6 Image pull secrets: <none> Mountable secrets: fluent-bit-token-pgpss Tokens: fluent-bit-token-pgpss Events: <none> Provision an Elasticsearch cluster: Provision a public Elasticsearch cluster with Fine-Grained Access Control enabled and a built-in user database: $ cat <<EOF> ~/environment/logging/elasticsearch_domain.json { \"DomainName\": ${ES_DOMAIN_NAME}, \"ElasticsearchVersion\": ${ES_VERSION}, \"ElasticsearchClusterConfig\": { \"InstanceType\": \"r5.large.elasticsearch\", \"InstanceCount\": 1, \"DedicatedMasterEnabled\": false, \"ZoneAwarenessEnabled\": false, \"WarmEnabled\": false }, \"EBSOptions\": { \"EBSEnabled\": true, \"VolumeType\": \"gp2\", \"VolumeSize\": 100 }, \"AccessPolicies\": \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"AWS\\\":\\\"*\\\"},\\\"Action\\\":\\\"es:ESHttp*\\\",\\\"Resource\\\":\\\"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}/*\\\"}]}\", \"SnapshotOptions\": {}, \"CognitoOptions\": { \"Enabled\": false }, \"EncryptionAtRestOptions\": { \"Enabled\": true }, \"NodeToNodeEncryptionOptions\": { \"Enabled\": true }, \"DomainEndpointOptions\": { \"EnforceHTTPS\": true, \"TLSSecurityPolicy\": \"Policy-Min-TLS-1-0-2019-07\" }, \"AdvancedSecurityOptions\": { \"Enabled\": true, \"InternalUserDatabaseEnabled\": true, \"MasterUserOptions\": { \"MasterUserName\": ${ES_USER}, \"MasterUserPassword\": ${ES_PASSWORD} } } } EOF $ aws es create-elasticsearch-domain \\ --cli-input-json file://~/environment/logging/es_domain.json It takes a while for Elasticsearch clusters to change to an active state. Check the AWS Console to see the status of the cluster, and continue to the next step when the cluster is ready. Configure Elasticsearch access: At this point you need to map roles to users in order to set fine-grained access control, because without this mapping all the requests to the cluster will result in permission errors. You should add the Fluent Bit ARN as a backend role to the all-access role, which uses the Elasticsearch APIs. To find the Fluent Bit ARN run the following command and export the value of ARN Role into the FLUENTBIT_ROLE environment variable: $ eksctl get iamserviceaccount --cluster dcd-ap-southeast-2 [ \u2139 ] eksctl version 0 .37.0 [ \u2139 ] using region ap-southeast-2 NAMESPACE NAME ROLE ARN kube-system cluster-autoscaler arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E $ export FLUENTBIT_ROLE = arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E Retrieve the Elasticsearch endpoint and update the internal database: $ export ES_ENDPOINT = $( aws es describe-elasticsearch-domain --domain-name ngh-search-domain --output text --query \"DomainStatus.Endpoint\" ) $ curl -sS -u \" ${ ES_DOMAIN_USER } : ${ ES_DOMAIN_PASSWORD } \" \\ -X PATCH \\ https:// ${ ES_ENDPOINT } /_opendistro/_security/api/rolesmapping/all_access?pretty \\ -H 'Content-Type: application/json' \\ -d ' [ { \"op\": \"add\", \"path\": \"/backend_roles\", \"value\": [\"' ${ FLUENTBIT_ROLE } '\"] } ] ' Finally, it is time to deploy Fluent Bit DaemonSet: $ kubectl apply -f src/main/logging/fluentbit.yaml After a few minutes all pods should be up and in running status. This is the end of the, you can open Kibana to visualise the logs. The endpoint for Kibana can be found in the Elasticsearch output tab in the AWS console, or you can run the following command: $ echo \"Kibana URL: https:// ${ ES_ENDPOINT } /_plugin/kibana/\" Kibana URL: https://search-domain-uehlb3kxledxykchwexee.ap-southeast-2.es.amazonaws.com/_plugin/kibana/ The user and password for Kibana are the same as the master user credential that is set in Elasticsearch in the provisioning stage. Open Kibana in a browser and after login, create an index pattern and see the report in the Discover page.","title":"Managed Elasticsearch in AWS"},{"location":"examples/ssh/SSH_BITBUCKET/","text":"SSH service in Bitbucket on Kubernetes \u00b6 In addition to providing a service on HTTP(S), Bitbucket also allows remote Git operations over SSH connections. By default, Kubernetes Ingress controllers only work for HTTP connections, but some ingress controllers also support TCP connections. Depending on the need of your deployment, SSH access can be provided through two mechanisms: Opening the TCP port through the ingress controller - This option should be used if the SSH service is required to be available on the same DNS name as the HTTP service. Creating a separate Kubernetes LoadBalancer service - This option is available if the ingress controller does not support TCP connections, or if you don\u2019t need your deployment to have the SSH service available on the same DNS name as the HTTP service. NGINX Ingress controller config for SSH connections \u00b6 We can follow the official documentation for the NGINX Ingress controller for this: Exposing TCP and UDP services - NGINX Ingress Controller . NOTE: These instructions should be performed in the same namespace in which the Ingress controller resides. 1. Create ConfigMap \u00b6 Create a new ConfigMap : kubectl create configmap tcp-services In our example we deployed Bitbucket under the name bitbucket in the namespace ssh-test , update the ConfigMap tcp-services accordingly: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 7999 : \"ssh-test/bitbucket:ssh\" 2. Update Ingress deployment \u00b6 Next, we have to edit the deployment of the ingress controller and add the --tcp-services-configmap option: kubectl edit deployment <name of ingress-nginx deployment> Add this line in the args of the container spec : - --tcp-services-configmap = $( POD_NAMESPACE ) /tcp-services so it looks something like this: spec : containers : - args : - /nginx-ingress-controller - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services 3. Update the Ingress service \u00b6 Update the Ingress service to include an additional port definition for ssh kubectl edit service <name of ingress-nginx service> Add this section in the ports of the container spec : - name : ssh port : 7999 protocol : TCP so it looks something like this: spec : clusterIP : 10.100.19.60 externalTrafficPolicy : Cluster ports : - name : http nodePort : 31381 port : 80 protocol : TCP targetPort : http - name : https nodePort : 32612 port : 443 protocol : TCP targetPort : https - name : ssh port : 7999 protocol : TCP After the deployment has been upgraded, the SSH service should be available on port 7999 . LoadBalancer service for SSH connections on AWS \u00b6 In the values file for the helm chart, the extra SSH service can be enabled like this: bitbucket : sshService : enabled : true On a deployment using AWS, assuming you have external-dns configured, you can add these annotations to automatically set up the DNS name for the SSH service: bitbucket : sshService : enabled : true annotations : external-dns.alpha.kubernetes.io/hostname : bitbucket-ssh.example.com additionalEnvironmentVariables : - name : PLUGIN_SSH_BASEURL value : ssh://bitbucket-ssh.example.com/","title":"SSH service in Bitbucket on Kubernetes"},{"location":"examples/ssh/SSH_BITBUCKET/#ssh-service-in-bitbucket-on-kubernetes","text":"In addition to providing a service on HTTP(S), Bitbucket also allows remote Git operations over SSH connections. By default, Kubernetes Ingress controllers only work for HTTP connections, but some ingress controllers also support TCP connections. Depending on the need of your deployment, SSH access can be provided through two mechanisms: Opening the TCP port through the ingress controller - This option should be used if the SSH service is required to be available on the same DNS name as the HTTP service. Creating a separate Kubernetes LoadBalancer service - This option is available if the ingress controller does not support TCP connections, or if you don\u2019t need your deployment to have the SSH service available on the same DNS name as the HTTP service.","title":"SSH service in Bitbucket on Kubernetes"},{"location":"examples/ssh/SSH_BITBUCKET/#nginx-ingress-controller-config-for-ssh-connections","text":"We can follow the official documentation for the NGINX Ingress controller for this: Exposing TCP and UDP services - NGINX Ingress Controller . NOTE: These instructions should be performed in the same namespace in which the Ingress controller resides.","title":"NGINX Ingress controller config for SSH connections"},{"location":"examples/ssh/SSH_BITBUCKET/#1-create-configmap","text":"Create a new ConfigMap : kubectl create configmap tcp-services In our example we deployed Bitbucket under the name bitbucket in the namespace ssh-test , update the ConfigMap tcp-services accordingly: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 7999 : \"ssh-test/bitbucket:ssh\"","title":"1. Create ConfigMap"},{"location":"examples/ssh/SSH_BITBUCKET/#2-update-ingress-deployment","text":"Next, we have to edit the deployment of the ingress controller and add the --tcp-services-configmap option: kubectl edit deployment <name of ingress-nginx deployment> Add this line in the args of the container spec : - --tcp-services-configmap = $( POD_NAMESPACE ) /tcp-services so it looks something like this: spec : containers : - args : - /nginx-ingress-controller - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services","title":"2. Update Ingress deployment"},{"location":"examples/ssh/SSH_BITBUCKET/#3-update-the-ingress-service","text":"Update the Ingress service to include an additional port definition for ssh kubectl edit service <name of ingress-nginx service> Add this section in the ports of the container spec : - name : ssh port : 7999 protocol : TCP so it looks something like this: spec : clusterIP : 10.100.19.60 externalTrafficPolicy : Cluster ports : - name : http nodePort : 31381 port : 80 protocol : TCP targetPort : http - name : https nodePort : 32612 port : 443 protocol : TCP targetPort : https - name : ssh port : 7999 protocol : TCP After the deployment has been upgraded, the SSH service should be available on port 7999 .","title":"3. Update the Ingress service"},{"location":"examples/ssh/SSH_BITBUCKET/#loadbalancer-service-for-ssh-connections-on-aws","text":"In the values file for the helm chart, the extra SSH service can be enabled like this: bitbucket : sshService : enabled : true On a deployment using AWS, assuming you have external-dns configured, you can add these annotations to automatically set up the DNS name for the SSH service: bitbucket : sshService : enabled : true annotations : external-dns.alpha.kubernetes.io/hostname : bitbucket-ssh.example.com additionalEnvironmentVariables : - name : PLUGIN_SSH_BASEURL value : ssh://bitbucket-ssh.example.com/","title":"LoadBalancer service for SSH connections on AWS"},{"location":"examples/storage/STORAGE/","text":"Shared storage \u00b6 Atlassian's Data Center products require a shared storage solution to effectively operate in multi-node environment. The specifics of how this shared storage is created is site-dependent, we do however provide examples on how shared storage can be created below. NOTE: Of all the Atlassian products, Bitbucket's shared storage solution must be NFS based. See Bitbucket NFS below for details. AWS EFS \u00b6 Jira, Confluence and Crowd can all be configured with an EFS-backed shared solution. For details on how this can be set up, see the AWS EFS example . NFS \u00b6 For details on creating shared storage for Bitbucket, see the NFS example .","title":"Shared storage"},{"location":"examples/storage/STORAGE/#shared-storage","text":"Atlassian's Data Center products require a shared storage solution to effectively operate in multi-node environment. The specifics of how this shared storage is created is site-dependent, we do however provide examples on how shared storage can be created below. NOTE: Of all the Atlassian products, Bitbucket's shared storage solution must be NFS based. See Bitbucket NFS below for details.","title":"Shared storage"},{"location":"examples/storage/STORAGE/#aws-efs","text":"Jira, Confluence and Crowd can all be configured with an EFS-backed shared solution. For details on how this can be set up, see the AWS EFS example .","title":"AWS EFS"},{"location":"examples/storage/STORAGE/#nfs","text":"For details on creating shared storage for Bitbucket, see the NFS example .","title":"NFS"},{"location":"examples/storage/aws/LOCAL_STORAGE/","text":"Local storage \u00b6 This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes. Dynamic provisioning \u00b6 Due to the ephemeral nature of Kubernetes pods we advise dynamic provisioning be used for creating and consuming EBS volume(s) Prerequisites \u00b6 EBS CSI driver is installed within the k8s cluster. Ensure that enableVolumeScheduling=true is set when installing, see here for additional details. You can confirm that the EBS CSI driver has been installed by running kubectl get csidriver Provisioning \u00b6 Create a Storage Class Update values.yaml to utilise Storage Class 1. Create Storage Class \u00b6 kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : ebs-sc provisioner : ebs.csi.aws.com volumeBindingMode : WaitForFirstConsumer 2. Update values.yaml \u00b6 Update the localHome storageClassName value within values.yaml to the name of the Storage Class created in step 1 above volumes : localHome : persistentVolumeClaim : create : true storageClassName : \"ebs-sc\" Resources \u00b6 Some useful resources on provisioning local storage with the AWS CSI Driver https://github.com/kubernetes-sigs/aws-ebs-csi-driver https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/","title":"Local storage"},{"location":"examples/storage/aws/LOCAL_STORAGE/#local-storage","text":"This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes.","title":"Local storage"},{"location":"examples/storage/aws/LOCAL_STORAGE/#dynamic-provisioning","text":"Due to the ephemeral nature of Kubernetes pods we advise dynamic provisioning be used for creating and consuming EBS volume(s)","title":"Dynamic provisioning"},{"location":"examples/storage/aws/LOCAL_STORAGE/#prerequisites","text":"EBS CSI driver is installed within the k8s cluster. Ensure that enableVolumeScheduling=true is set when installing, see here for additional details. You can confirm that the EBS CSI driver has been installed by running kubectl get csidriver","title":"Prerequisites"},{"location":"examples/storage/aws/LOCAL_STORAGE/#provisioning","text":"Create a Storage Class Update values.yaml to utilise Storage Class","title":"Provisioning"},{"location":"examples/storage/aws/LOCAL_STORAGE/#1-create-storage-class","text":"kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : ebs-sc provisioner : ebs.csi.aws.com volumeBindingMode : WaitForFirstConsumer","title":"1. Create Storage Class"},{"location":"examples/storage/aws/LOCAL_STORAGE/#2-update-valuesyaml","text":"Update the localHome storageClassName value within values.yaml to the name of the Storage Class created in step 1 above volumes : localHome : persistentVolumeClaim : create : true storageClassName : \"ebs-sc\"","title":"2. Update values.yaml"},{"location":"examples/storage/aws/LOCAL_STORAGE/#resources","text":"Some useful resources on provisioning local storage with the AWS CSI Driver https://github.com/kubernetes-sigs/aws-ebs-csi-driver https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/","title":"Resources"},{"location":"examples/storage/aws/SHARED_STORAGE/","text":"Shared storage \u00b6 This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize an AWS EFS backed filesystem. Static provisioning \u00b6 An example detailing how an existing EFS filesystem can be created and consumed using static provisioning. Prerequisites \u00b6 EFS CSI driver is installed within the k8s cluster. A physical EFS filesystem has been provisioned Additional details on static EFS provisioning can be found here Confirm that the EFS CSI driver has been installed by running kubectl get csidriver Provisioning \u00b6 Create a Persistent Volume Create a Persistent Volume Claim Update values.yaml to utilise Persistent Volume Claim 1. Create Persistent Volume \u00b6 Create a persistent volume for the pre-provisioned EFS filesystem by providing the <efs-id> . The EFS id can be identified using the CLI command below with the appropriate region aws efs describe-file-systems --query \"FileSystems[*].FileSystemId\" --region ap-southeast-2 apiVersion : v1 kind : PersistentVolume metadata : name : my-shared-vol-pv spec : capacity : storage : 1Gi volumeMode : Filesystem accessModes : - ReadWriteMany storageClassName : efs-pv persistentVolumeReclaimPolicy : Retain mountOptions : - rw - lookupcache=pos - noatime - intr - _netdev csi : driver : efs.csi.aws.com volumeHandle : <efs-id> 2. Create Persistent Volume Claim \u00b6 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : my-shared-vol-pvc spec : accessModes : - ReadWriteMany storageClassName : efs-pv volumeMode : Filesystem volumeName : my-shared-vol-pv resources : requests : storage : 1Gi 3. Update values.yaml \u00b6 Update the sharedHome claimName value within values.yaml to the name of the Persistent Volume Claim created in step 2 above volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : \"my-shared-vol-pvc\" Resources \u00b6 Some useful resources on provisioning shared storage with the AWS CSI Driver: Amazon EFS CSI driver Introducing Amazon EFS CSI dynamic provisioning","title":"Shared storage"},{"location":"examples/storage/aws/SHARED_STORAGE/#shared-storage","text":"This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize an AWS EFS backed filesystem.","title":"Shared storage"},{"location":"examples/storage/aws/SHARED_STORAGE/#static-provisioning","text":"An example detailing how an existing EFS filesystem can be created and consumed using static provisioning.","title":"Static provisioning"},{"location":"examples/storage/aws/SHARED_STORAGE/#prerequisites","text":"EFS CSI driver is installed within the k8s cluster. A physical EFS filesystem has been provisioned Additional details on static EFS provisioning can be found here Confirm that the EFS CSI driver has been installed by running kubectl get csidriver","title":"Prerequisites"},{"location":"examples/storage/aws/SHARED_STORAGE/#provisioning","text":"Create a Persistent Volume Create a Persistent Volume Claim Update values.yaml to utilise Persistent Volume Claim","title":"Provisioning"},{"location":"examples/storage/aws/SHARED_STORAGE/#1-create-persistent-volume","text":"Create a persistent volume for the pre-provisioned EFS filesystem by providing the <efs-id> . The EFS id can be identified using the CLI command below with the appropriate region aws efs describe-file-systems --query \"FileSystems[*].FileSystemId\" --region ap-southeast-2 apiVersion : v1 kind : PersistentVolume metadata : name : my-shared-vol-pv spec : capacity : storage : 1Gi volumeMode : Filesystem accessModes : - ReadWriteMany storageClassName : efs-pv persistentVolumeReclaimPolicy : Retain mountOptions : - rw - lookupcache=pos - noatime - intr - _netdev csi : driver : efs.csi.aws.com volumeHandle : <efs-id>","title":"1. Create Persistent Volume"},{"location":"examples/storage/aws/SHARED_STORAGE/#2-create-persistent-volume-claim","text":"apiVersion : v1 kind : PersistentVolumeClaim metadata : name : my-shared-vol-pvc spec : accessModes : - ReadWriteMany storageClassName : efs-pv volumeMode : Filesystem volumeName : my-shared-vol-pv resources : requests : storage : 1Gi","title":"2. Create Persistent Volume Claim"},{"location":"examples/storage/aws/SHARED_STORAGE/#3-update-valuesyaml","text":"Update the sharedHome claimName value within values.yaml to the name of the Persistent Volume Claim created in step 2 above volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : \"my-shared-vol-pvc\"","title":"3. Update values.yaml"},{"location":"examples/storage/aws/SHARED_STORAGE/#resources","text":"Some useful resources on provisioning shared storage with the AWS CSI Driver: Amazon EFS CSI driver Introducing Amazon EFS CSI dynamic provisioning","title":"Resources"},{"location":"examples/storage/nfs/NFS/","text":"Implementation of an NFS Server for Bitbucket \u00b6 Warning This functionality is not officially supported. The included examples are provided as is and are to be used as guidance on how to set up a testing environment. These examples should not be used in production. Before you proceed we highly recommend that you understand your specific deployment needs and tailor your solution to them. Components \u00b6 For a full production deployment you will need to create some common components. These components are then passed as values to the Helm chart when you install your product. The components are: Shared storage Database Elasticsearch Shared storage \u00b6 Cloud-managed shared storage \u00b6 Dedicated NFS server - Bitbucket Data Center requirement \u00b6 Bitbucket Data Center uses a shared network file system (NFS) to store its repositories in a common location that is accessible to multiple Bitbucket nodes. Due to the high requirements on performance for IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. Based on this, we don't recommend that you use cloud managed storage services . You might choose to use an NFS server for other Data Center products, but they don't have the same performance characteristics. It might be better to go for the resilience of a managed service over a self-managed server for other products. Requirements \u00b6 Prior to installing the Helm chart, you need to provision a suitable NFS shared storage solution. The exact details of this resource will be highly site-specific, but the example below can be used as a guide. For more information on setting up Bitbucket Data Center's shared file server, see Step 2. Provision your shared file system . This section contains the requirements and recommendations for setting up NFS for Bitbucket Data Center. You need to set your NFS server's size according to your instance needs. See the capacity recommendations](https://confluence.atlassian.com/bitbucketserver/recommendations-for-running-bitbucket-in-aws-776640282.html). Example \u00b6 We've provided a template as a reference on how an NFS server could be stood-up to work in conjunction with a Bitbucket deployment: nfs-server-example . Provision the NFS by using the following command: helm install nfs-server-example nfs-server-example :warning: Please note that the NFS server created with this template is not production ready and should not be used for anything other than testing deployment. Pod affinity \u00b6 We highly recommend to keep NFS server and Bitbucket nodes in close proximity. To achieve this, you can use standard Kubernetes affinity rules . Use the suitable affinity definition in the affinity: {} definition in the values.yaml file.","title":"Implementation of an NFS Server for Bitbucket"},{"location":"examples/storage/nfs/NFS/#implementation-of-an-nfs-server-for-bitbucket","text":"Warning This functionality is not officially supported. The included examples are provided as is and are to be used as guidance on how to set up a testing environment. These examples should not be used in production. Before you proceed we highly recommend that you understand your specific deployment needs and tailor your solution to them.","title":"Implementation of an NFS Server for Bitbucket"},{"location":"examples/storage/nfs/NFS/#components","text":"For a full production deployment you will need to create some common components. These components are then passed as values to the Helm chart when you install your product. The components are: Shared storage Database Elasticsearch","title":"Components"},{"location":"examples/storage/nfs/NFS/#shared-storage","text":"","title":"Shared storage"},{"location":"examples/storage/nfs/NFS/#cloud-managed-shared-storage","text":"","title":"Cloud-managed shared storage"},{"location":"examples/storage/nfs/NFS/#dedicated-nfs-server-bitbucket-data-center-requirement","text":"Bitbucket Data Center uses a shared network file system (NFS) to store its repositories in a common location that is accessible to multiple Bitbucket nodes. Due to the high requirements on performance for IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. Based on this, we don't recommend that you use cloud managed storage services . You might choose to use an NFS server for other Data Center products, but they don't have the same performance characteristics. It might be better to go for the resilience of a managed service over a self-managed server for other products.","title":"Dedicated NFS server - Bitbucket Data Center requirement"},{"location":"examples/storage/nfs/NFS/#requirements","text":"Prior to installing the Helm chart, you need to provision a suitable NFS shared storage solution. The exact details of this resource will be highly site-specific, but the example below can be used as a guide. For more information on setting up Bitbucket Data Center's shared file server, see Step 2. Provision your shared file system . This section contains the requirements and recommendations for setting up NFS for Bitbucket Data Center. You need to set your NFS server's size according to your instance needs. See the capacity recommendations](https://confluence.atlassian.com/bitbucketserver/recommendations-for-running-bitbucket-in-aws-776640282.html).","title":"Requirements"},{"location":"examples/storage/nfs/NFS/#example","text":"We've provided a template as a reference on how an NFS server could be stood-up to work in conjunction with a Bitbucket deployment: nfs-server-example . Provision the NFS by using the following command: helm install nfs-server-example nfs-server-example :warning: Please note that the NFS server created with this template is not production ready and should not be used for anything other than testing deployment.","title":"Example"},{"location":"examples/storage/nfs/NFS/#pod-affinity","text":"We highly recommend to keep NFS server and Bitbucket nodes in close proximity. To achieve this, you can use standard Kubernetes affinity rules . Use the suitable affinity definition in the affinity: {} definition in the values.yaml file.","title":"Pod affinity"},{"location":"installation/CONFIGURATION/","text":"Configuration \u00b6 Ingress \u00b6 In order to make the Atlassian product available from outside of the Kubernetes cluster, a suitable HTTP/HTTPS ingress controller needs to be installed. The standard Kubernetes Ingress resource is not flexible enough for our needs, so a third-party ingress controller and resource definition must be provided. The exact details of the Ingress will be highly site-specific. These Helm charts were tested using the NGINX Ingress Controller . We also provide example instructions on how this controller can be installed and configured. The charts themselves provide a template for Ingress resource rules to be utilised by the provisioned controller. These include all required annotations and optional TLS configuration for the NGINX Ingress Controller. Some key considerations to note when configuring the controller are: Ingress At a minimum, the ingress needs the ability to support long request timeouts, as well as session affinity (aka \"sticky sessions\"). The Ingress Resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml . Some key aspects that can be configured include: Usage of the NGINX Ingress Controller Ingress Controller annotations The request max body size The hostname of the ingress resource When installed, with the provided configuration , the NGINX Ingress Controller will provision an internet-facing (see diagram below) load balancer on your behalf. The load balancer should either support the Proxy Protocol or allow for the forwarding of X-Forwarded-* headers. This ensures any backend redirects are done so over the correct protocol. If the X-Forwarded-* headers are being used, then enable the use-forwarded-headers option on the controllers ConfigMap . This ensures that these headers are appropriately passed on. The diagram below provides a high-level overview of how external requests are routed via an internet-facing load balancer to the correct service via Ingress. Traffic flow (diagram) Inbound client request DNS routes request to appropriate LB LB forwards request to internal Ingress Ingress controller performs traffic routing lookup via Ingress object(s) Ingress forwards request to appropriate service based on Ingress object routing rule Service forwards request to appropriate pod Pod handles request Volumes \u00b6 The Data Center products make use of filesystem storage. Each DC node has its own \"local-home\" volume, and all nodes in the DC cluster share a single \"shared-home\" volume. By default, the Helm charts will configure all of these volumes as ephemeral \"emptyDir\" volumes. This makes it possible to install the charts without configuring any volume management, but comes with two big caveats: Any data stored in the local-home or shared-home will be lost every time a pod starts. Whilst the data that is stored in local-home can generally be regenerated (e.g. from the database), this can be very a very expensive process that sometimes required manual intervention. The shared-home volume will not actually be shared between multiple nodes in the DC cluster. Whilst this may not immediately prevent scaling the DC cluster up to multiple nodes, certain critical functionality of the products relies on the shared filesystem working as expected. For these reasons, the default volume configuration of the Helm charts is suitable only for running a single DC node for evaluation purposes. Proper volume management needs to be configured in order for the data to survive restarts, and for multi-node DC clusters to operate correctly. While you are free to configure your Kubernetes volume management in any way you wish, within the constraints imposed by the products, the recommended setup is to use Kubernetes PersistentVolumes and PersistentVolumeClaims . The local-home volume requires a PersistentVolume with ReadWriteOnce (RWO) capability, and shared-home requires a PersistentVolume with ReadWriteMany (RWX) capability. Typically, this will be an NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own RWX volumes (e.g. AzureFile, ElasticFileStore). While this entails a higher upfront setup effort, it gives the best flexibility. Volumes configuration \u00b6 By default, the charts will configure the local-home and shared-home values as follows: volumes : - name : local-home emptyDir : {} - name : shared-home emptyDir : {} As explained above, this default configuration is suitable only for testing purposes. Proper volume management needs to be configured. In order to enable the persistence of data stored in these volumes, it is necessary to replace these volumes with something else. The recommended way is to enable the use of PersistentVolume and PersistentVolumeClaim for both volumes, using your install-specific values.yaml file, for example: volumes : localHome : persistentVolumeClaim : create : true shared-home : persistentVolumeClaim : create : true This will result in each pod in the StatefulSet creating a local-home PersistentVolumeClaim of type ReadWriteOnce , and a single PersistentVolumeClaim of type ReadWriteMany being created for the shared-home . For each PersistentVolumeClaim created by the chart, a suitable PersistentVolume needs to be made available prior to installation. These can be provisioned either statically or dynamically, using an auto-provisioner. An alternative to PersistentVolumeClaims is to use inline volume definitions, either for local-home or shared-home (or both), for example: volumes : localHome : customVolume : hostPath : path : /path/to/my/data shared-home : customVolume : nfs : server : mynfsserver path : /export/path Generally, any valid Kubernetes volume resource definition can be substituted here. However, as mentioned previously, externalising the volume definitions using PersistentVolumes is the strongly recommended approach. Volumes examples \u00b6 Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket . We have an example detailing how an existing EFS filesystem can be created and consumed using static provisioning: Shared storage - utilizing AWS EFS-backed filesystem . You can also refer to an example on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes: Local storage - utilizing AWS EBS-backed volumes . Additional volumes \u00b6 In additional to the local-home and shared-home volumes that are always attached to the product pods, you can attach your own volumes for your own purposes, and mount them into the product container. Use the additional (under volumes ) and additionalVolumeMounts values to both attach the volumes and mount them in to the product container. This might be useful if, for example, you have a custom plugin that requires its own filesystem storage. Example: jira : additionalVolumeMounts : - volumeName : my-volume mountPath : /path/to/mount volumes : additional : - name : my-volume persistentVolumeClaim : claimName : my-volume-claim Database connectivity \u00b6 The products need to be supplied with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. database.url \u00b6 All products require the JDBC URL of the database. The format if this URL depends on the JDBC driver being used, but some examples are: Vendor JDBC driver class Example JDBC URL Postgres org.postgresql.Driver jdbc:postgresql://<dbhost>:5432/<dbname> MySQL com.mysql.jdbc.Driver jdbc:mysql://<dbhost>/<dbname> SQL Server com.microsoft.sqlserver.jdbc.SQLServerDriver jdbc:sqlserver://<dbhost>:1433;databaseName=<dbname> Oracle oracle.jdbc.OracleDriver jdbc:oracle:thin:@<dbhost>:1521:<SID> NOTE: With regards, <dbname> in the JDBC URL , this database is not automatically created by the Atlassian product itself, as such a user and database must be manually created for the DB instance used. Details on how to create product specific DB's can be found below: * Jira * Confluence * Bitbucket * Crowd database.driver \u00b6 Jira and Bitbucket require the JDBC driver class to be specified (Confluence will autoselect this based on the database.type value, see below). The JDBC driver must correspond to the JDBC URL used; see the table above for example driver classes. Note that the products only ship with certain JDBC drivers installed, depending on the license conditions of those drivers. In order to use JDBC drivers that are not shipped with the product (e.g. MySQL and Oracle), you need to follow the steps to introduce additional libraries into the installation (see below). database.type \u00b6 Jira and Confluence both require this value to be specified, this declares the database engine to be used. The acceptable values for this include: Vendor Jira Confluence Postgres postgres72 postgresql MySQL mysql57 / mysql8 mysql SQL Server mssql mssql Oracle oracle10g oracle Database credentials \u00b6 All products can have their database connectivity and credentials specified either interactively during first-time setup, or automatically by specifying certain configuration via Kubernetes. Depending on the product, the database.type , database.url and database.driver chart values can be provided. In addition, the database username and password can be provided via a Kubernetes secret, with the secret name specified with the database.credentials.secretName chart value. When all the required information is provided in this way, the database connectivity configuration screen will be bypassed during product setup. Namespace \u00b6 The Helm charts are not opinionated as to whether they have a Kubernetes namespace to themselves. If you wish, you can run multiple Helm releases of the same product in the same namespace. Clustering \u00b6 By default, the Helm charts are will not configure the products for Data Center clustering. In order to enable clustering, the appropriate chart value must be set to true . Jira jira.clustering.enabled Confluence confluence.clustering.enabled Bitbucket bitbucket.clustering.enabled In addition, the shared-home volume must be correctly configured as a read-write shared filesystem (e.g. NFS, AWS EFS, Azure Files) Additional libraries & plugins \u00b6 The products' Docker images contain the default set of bundled libraries and plugins. Additional libraries and plugins can be mounted into the product containers during the Helm install. This can be useful for bundling extra JDBC drivers, for example, or additional bundled plugins that you need for your installation. In order for this to work, the additional JAR files need to be available as part of a Kubernetes volume. Options here include putting them into the shared-home volume that you already need to have as part of the installation. Alternatively, you can create a new PV for them, as long as it has ReadOnlyMany capability. You could even store the files as a ConfigMap that gets mounted as a volume, but you're likely to run into file size limitations there. Assuming that the existing shared-home volume is used for this, then the only configuration required is to specify the additionalLibraries and/or additionalBundledPlugins structures in your values.yaml file, e.g. jira : additionalLibraries : - volumeName : shared-home subDirectory : mylibs fileName : lib1.jar - volumeName : shared-home subDirectory : mylibs fileName : lib2.jar This will mount the lib1.jar and lib2.jar in the appropriate place in the container. Similarly, use additionalBundledPlugins to load product plugins into the container. Note: Plugins installed via this method will appear as system plugins rather than user plugins. An alternative to this method is to install the plugins via \"Manage Apps\" in the product system administration UI. If you're not using the shared-home volume, then you can declare your own custom volume in the \"Additional Volumes\" section below, then declare the libraries as above (but with your custom volume name). Request body size \u00b6 By default the maximum allowed size for the request body is set to 250MB. If the size in a request exceeds the maximum size of the client request body, an 413 error will be returned to the client. If the maximum request body can be configured by changing the value of maxBodySize in 'values.yaml'. Resources \u00b6 The Helm charts allow you to specify container-level CPU and memory resources, using standard Kubernetes limit and request structures, e.g. jira : resources : container : requests : cpu : \"4\" memory : \"8G\" By default, no container-level resource limits or requests are set. Specifying these values is fine for CPU limits/requests, but for memory resources it is also necessary to configure the JVM's memory limits. By default, the JVM maximum heap size is set to 1 GB, so if you increase (or decrease) the container memory resources as above, you also need to change the JVM's max heap size, otherwise the JVM won't take advantage of the extra available memory (or it'll crash if there isn't enough). You specify the JVM memory limits like this: jira : resources : jvm : maxHeap : \"8g\" Another difficulty for specifying memory resources is that the JVM requires additional overheads over and above the max heap size, and the container resources need to take account of that. A safe rule-of-thumb would be for the container to request 2x the value of the max heap for the JVM. This requirement to configure both the container memory and JVM heap will hopefully be removed. You can read more about resource scaling and resource requests and limits . Additional containers \u00b6 The Helm charts allow you to add your own container and initContainer entries to the product pods. Use the values additionalContainers and additionalInitContainers for this. One use-case for an additional container would be to attach a sidecar container to the product pods. Additional labels, toleration's, node selectors, affinity \u00b6 The Helm charts also allow you to specify additionalLabls , tolerations , nodeSelectors and affinities . These are standard Kubernetes structures that will be included in the pods.","title":"Configuration"},{"location":"installation/CONFIGURATION/#configuration","text":"","title":"Configuration"},{"location":"installation/CONFIGURATION/#ingress","text":"In order to make the Atlassian product available from outside of the Kubernetes cluster, a suitable HTTP/HTTPS ingress controller needs to be installed. The standard Kubernetes Ingress resource is not flexible enough for our needs, so a third-party ingress controller and resource definition must be provided. The exact details of the Ingress will be highly site-specific. These Helm charts were tested using the NGINX Ingress Controller . We also provide example instructions on how this controller can be installed and configured. The charts themselves provide a template for Ingress resource rules to be utilised by the provisioned controller. These include all required annotations and optional TLS configuration for the NGINX Ingress Controller. Some key considerations to note when configuring the controller are: Ingress At a minimum, the ingress needs the ability to support long request timeouts, as well as session affinity (aka \"sticky sessions\"). The Ingress Resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml . Some key aspects that can be configured include: Usage of the NGINX Ingress Controller Ingress Controller annotations The request max body size The hostname of the ingress resource When installed, with the provided configuration , the NGINX Ingress Controller will provision an internet-facing (see diagram below) load balancer on your behalf. The load balancer should either support the Proxy Protocol or allow for the forwarding of X-Forwarded-* headers. This ensures any backend redirects are done so over the correct protocol. If the X-Forwarded-* headers are being used, then enable the use-forwarded-headers option on the controllers ConfigMap . This ensures that these headers are appropriately passed on. The diagram below provides a high-level overview of how external requests are routed via an internet-facing load balancer to the correct service via Ingress. Traffic flow (diagram) Inbound client request DNS routes request to appropriate LB LB forwards request to internal Ingress Ingress controller performs traffic routing lookup via Ingress object(s) Ingress forwards request to appropriate service based on Ingress object routing rule Service forwards request to appropriate pod Pod handles request","title":"Ingress"},{"location":"installation/CONFIGURATION/#volumes","text":"The Data Center products make use of filesystem storage. Each DC node has its own \"local-home\" volume, and all nodes in the DC cluster share a single \"shared-home\" volume. By default, the Helm charts will configure all of these volumes as ephemeral \"emptyDir\" volumes. This makes it possible to install the charts without configuring any volume management, but comes with two big caveats: Any data stored in the local-home or shared-home will be lost every time a pod starts. Whilst the data that is stored in local-home can generally be regenerated (e.g. from the database), this can be very a very expensive process that sometimes required manual intervention. The shared-home volume will not actually be shared between multiple nodes in the DC cluster. Whilst this may not immediately prevent scaling the DC cluster up to multiple nodes, certain critical functionality of the products relies on the shared filesystem working as expected. For these reasons, the default volume configuration of the Helm charts is suitable only for running a single DC node for evaluation purposes. Proper volume management needs to be configured in order for the data to survive restarts, and for multi-node DC clusters to operate correctly. While you are free to configure your Kubernetes volume management in any way you wish, within the constraints imposed by the products, the recommended setup is to use Kubernetes PersistentVolumes and PersistentVolumeClaims . The local-home volume requires a PersistentVolume with ReadWriteOnce (RWO) capability, and shared-home requires a PersistentVolume with ReadWriteMany (RWX) capability. Typically, this will be an NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own RWX volumes (e.g. AzureFile, ElasticFileStore). While this entails a higher upfront setup effort, it gives the best flexibility.","title":"Volumes"},{"location":"installation/CONFIGURATION/#volumes-configuration","text":"By default, the charts will configure the local-home and shared-home values as follows: volumes : - name : local-home emptyDir : {} - name : shared-home emptyDir : {} As explained above, this default configuration is suitable only for testing purposes. Proper volume management needs to be configured. In order to enable the persistence of data stored in these volumes, it is necessary to replace these volumes with something else. The recommended way is to enable the use of PersistentVolume and PersistentVolumeClaim for both volumes, using your install-specific values.yaml file, for example: volumes : localHome : persistentVolumeClaim : create : true shared-home : persistentVolumeClaim : create : true This will result in each pod in the StatefulSet creating a local-home PersistentVolumeClaim of type ReadWriteOnce , and a single PersistentVolumeClaim of type ReadWriteMany being created for the shared-home . For each PersistentVolumeClaim created by the chart, a suitable PersistentVolume needs to be made available prior to installation. These can be provisioned either statically or dynamically, using an auto-provisioner. An alternative to PersistentVolumeClaims is to use inline volume definitions, either for local-home or shared-home (or both), for example: volumes : localHome : customVolume : hostPath : path : /path/to/my/data shared-home : customVolume : nfs : server : mynfsserver path : /export/path Generally, any valid Kubernetes volume resource definition can be substituted here. However, as mentioned previously, externalising the volume definitions using PersistentVolumes is the strongly recommended approach.","title":"Volumes configuration"},{"location":"installation/CONFIGURATION/#volumes-examples","text":"Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket . We have an example detailing how an existing EFS filesystem can be created and consumed using static provisioning: Shared storage - utilizing AWS EFS-backed filesystem . You can also refer to an example on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes: Local storage - utilizing AWS EBS-backed volumes .","title":"Volumes examples"},{"location":"installation/CONFIGURATION/#additional-volumes","text":"In additional to the local-home and shared-home volumes that are always attached to the product pods, you can attach your own volumes for your own purposes, and mount them into the product container. Use the additional (under volumes ) and additionalVolumeMounts values to both attach the volumes and mount them in to the product container. This might be useful if, for example, you have a custom plugin that requires its own filesystem storage. Example: jira : additionalVolumeMounts : - volumeName : my-volume mountPath : /path/to/mount volumes : additional : - name : my-volume persistentVolumeClaim : claimName : my-volume-claim","title":"Additional volumes"},{"location":"installation/CONFIGURATION/#database-connectivity","text":"The products need to be supplied with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences.","title":"Database connectivity"},{"location":"installation/CONFIGURATION/#databaseurl","text":"All products require the JDBC URL of the database. The format if this URL depends on the JDBC driver being used, but some examples are: Vendor JDBC driver class Example JDBC URL Postgres org.postgresql.Driver jdbc:postgresql://<dbhost>:5432/<dbname> MySQL com.mysql.jdbc.Driver jdbc:mysql://<dbhost>/<dbname> SQL Server com.microsoft.sqlserver.jdbc.SQLServerDriver jdbc:sqlserver://<dbhost>:1433;databaseName=<dbname> Oracle oracle.jdbc.OracleDriver jdbc:oracle:thin:@<dbhost>:1521:<SID> NOTE: With regards, <dbname> in the JDBC URL , this database is not automatically created by the Atlassian product itself, as such a user and database must be manually created for the DB instance used. Details on how to create product specific DB's can be found below: * Jira * Confluence * Bitbucket * Crowd","title":"database.url"},{"location":"installation/CONFIGURATION/#databasedriver","text":"Jira and Bitbucket require the JDBC driver class to be specified (Confluence will autoselect this based on the database.type value, see below). The JDBC driver must correspond to the JDBC URL used; see the table above for example driver classes. Note that the products only ship with certain JDBC drivers installed, depending on the license conditions of those drivers. In order to use JDBC drivers that are not shipped with the product (e.g. MySQL and Oracle), you need to follow the steps to introduce additional libraries into the installation (see below).","title":"database.driver"},{"location":"installation/CONFIGURATION/#databasetype","text":"Jira and Confluence both require this value to be specified, this declares the database engine to be used. The acceptable values for this include: Vendor Jira Confluence Postgres postgres72 postgresql MySQL mysql57 / mysql8 mysql SQL Server mssql mssql Oracle oracle10g oracle","title":"database.type"},{"location":"installation/CONFIGURATION/#database-credentials","text":"All products can have their database connectivity and credentials specified either interactively during first-time setup, or automatically by specifying certain configuration via Kubernetes. Depending on the product, the database.type , database.url and database.driver chart values can be provided. In addition, the database username and password can be provided via a Kubernetes secret, with the secret name specified with the database.credentials.secretName chart value. When all the required information is provided in this way, the database connectivity configuration screen will be bypassed during product setup.","title":"Database credentials"},{"location":"installation/CONFIGURATION/#namespace","text":"The Helm charts are not opinionated as to whether they have a Kubernetes namespace to themselves. If you wish, you can run multiple Helm releases of the same product in the same namespace.","title":"Namespace"},{"location":"installation/CONFIGURATION/#clustering","text":"By default, the Helm charts are will not configure the products for Data Center clustering. In order to enable clustering, the appropriate chart value must be set to true . Jira jira.clustering.enabled Confluence confluence.clustering.enabled Bitbucket bitbucket.clustering.enabled In addition, the shared-home volume must be correctly configured as a read-write shared filesystem (e.g. NFS, AWS EFS, Azure Files)","title":"Clustering"},{"location":"installation/CONFIGURATION/#additional-libraries-plugins","text":"The products' Docker images contain the default set of bundled libraries and plugins. Additional libraries and plugins can be mounted into the product containers during the Helm install. This can be useful for bundling extra JDBC drivers, for example, or additional bundled plugins that you need for your installation. In order for this to work, the additional JAR files need to be available as part of a Kubernetes volume. Options here include putting them into the shared-home volume that you already need to have as part of the installation. Alternatively, you can create a new PV for them, as long as it has ReadOnlyMany capability. You could even store the files as a ConfigMap that gets mounted as a volume, but you're likely to run into file size limitations there. Assuming that the existing shared-home volume is used for this, then the only configuration required is to specify the additionalLibraries and/or additionalBundledPlugins structures in your values.yaml file, e.g. jira : additionalLibraries : - volumeName : shared-home subDirectory : mylibs fileName : lib1.jar - volumeName : shared-home subDirectory : mylibs fileName : lib2.jar This will mount the lib1.jar and lib2.jar in the appropriate place in the container. Similarly, use additionalBundledPlugins to load product plugins into the container. Note: Plugins installed via this method will appear as system plugins rather than user plugins. An alternative to this method is to install the plugins via \"Manage Apps\" in the product system administration UI. If you're not using the shared-home volume, then you can declare your own custom volume in the \"Additional Volumes\" section below, then declare the libraries as above (but with your custom volume name).","title":"Additional libraries &amp; plugins"},{"location":"installation/CONFIGURATION/#request-body-size","text":"By default the maximum allowed size for the request body is set to 250MB. If the size in a request exceeds the maximum size of the client request body, an 413 error will be returned to the client. If the maximum request body can be configured by changing the value of maxBodySize in 'values.yaml'.","title":"Request body size"},{"location":"installation/CONFIGURATION/#resources","text":"The Helm charts allow you to specify container-level CPU and memory resources, using standard Kubernetes limit and request structures, e.g. jira : resources : container : requests : cpu : \"4\" memory : \"8G\" By default, no container-level resource limits or requests are set. Specifying these values is fine for CPU limits/requests, but for memory resources it is also necessary to configure the JVM's memory limits. By default, the JVM maximum heap size is set to 1 GB, so if you increase (or decrease) the container memory resources as above, you also need to change the JVM's max heap size, otherwise the JVM won't take advantage of the extra available memory (or it'll crash if there isn't enough). You specify the JVM memory limits like this: jira : resources : jvm : maxHeap : \"8g\" Another difficulty for specifying memory resources is that the JVM requires additional overheads over and above the max heap size, and the container resources need to take account of that. A safe rule-of-thumb would be for the container to request 2x the value of the max heap for the JVM. This requirement to configure both the container memory and JVM heap will hopefully be removed. You can read more about resource scaling and resource requests and limits .","title":"Resources"},{"location":"installation/CONFIGURATION/#additional-containers","text":"The Helm charts allow you to add your own container and initContainer entries to the product pods. Use the values additionalContainers and additionalInitContainers for this. One use-case for an additional container would be to attach a sidecar container to the product pods.","title":"Additional containers"},{"location":"installation/CONFIGURATION/#additional-labels-tolerations-node-selectors-affinity","text":"The Helm charts also allow you to specify additionalLabls , tolerations , nodeSelectors and affinities . These are standard Kubernetes structures that will be included in the pods.","title":"Additional labels, toleration's, node selectors, affinity"},{"location":"installation/INSTALLATION/","text":"Installation \u00b6 Follow these instructions to install your Atlassian product using the Helm charts. Before you proceed with the installation, make sure you have followed the Prerequisites guide . 1. Add the Helm chart repository \u00b6 Add the Helm chart repository to your local Helm installation: helm repo add atlassian-data-center \\ https://atlassian-labs.github.io/data-center-helm-charts Update the repo: helm repo update 2. Obtain values.yaml \u00b6 Obtain the default product values.yaml file from the chart: helm show values atlassian-data-center/<product> > values.yaml 3. Configure database \u00b6 Using the values.yaml file obtained in step 2 , configure the usage of the database provisioned as part of the prerequisites . By providing all the required database values, you will bypass the database connectivity configuration during the product setup. Migration If you are migrating an existing Data Center product to Kubernetes, use the values of your product's database. See Migration guide . Create a Kubernetes secret to store the connectivity details of the database: kubectl create secret generic <secret_name> --from-literal = username = '<db_username>' --from-literal = password = '<db_password>' Using the Kubernetes secret, update the database stanza within values.yaml appropriately. Refer to the commentary within the values.yaml file for additional details on how to configure the remaining database values: database : type : <db_type> url : <jdbc_url> driver : <engine_driver> credentials : secretName : <secret_name> usernameSecretKey : username passwordSecretKey : password For additional information on how the above values should be configured, see the Database connectivity section of the configuration guide . Read about Kubernetes secrets . 4. Configure Ingress \u00b6 Using the values.yaml file obtained in step 2 , configure the Ingress controller provisioned as part of the Prerequisites . The values you provide here will be used to provision an Ingress resource for the controller. Refer to the associated comments within the values.yaml file for additional details on how to configure the Ingress resource: ingress : create : true #1. Setting true here will create an Ingress resource nginx : true #2. If using the ingress-nginx controller set this property to true maxBodySize : 250m host : <dns_host_name> #2. Hosts can be precise matches (for example \u201cfoo.bar.com\u201d) or a wildcard (for example \u201c*.foo.com\u201d). path : \"/\" annotations : cert-manager.io/issuer : <certificate_issuer> https : true tlsSecretName : <tls_certificate_name> For additional details on Ingress controllers see the Ingress section of the configuration guide . See an example of how to set up a controller . 5. Configure persistent storage \u00b6 Using the values.yaml file obtained in step 2 , configure the shared-home that was provisioned as part of the Prerequisites . See shared home example . If you are migrating an existing Data Center product to Kubernetes , use the values of your product's shared home. volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : <pvc_name> Each pod will also require its own local-home storage. This can be configured with a StorageClass , as can be seen in the local home example . Having created the StorageClass , update values.yaml to make use of it: volumes : localHome : persistentVolumeClaim : create : true storageClassName : <storage-class-name> For more details, please refer to the Volumes section of the configuration guide . Bitbucket shared storage Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket . 6. Configure license and sysadmin credentials for Bitbucket \u00b6 Bitbucket is slightly different from the other products in that it can be completely configured during deployment, meaning no manual setup is required. To do this, you need to update the sysadminCredentials and license stanzas within the values.yaml obtained in step 2 . Create a Kubernetes secret to hold the Bitbucket license: kubectl create secret generic <license_secret_name> --from-literal = license-key = '<bitbucket_license_key>' Create a Kubernetes secret to hold the Bitbucket system administrator credentials: kubectl create secret generic <sysadmin_creds_secret_name> --from-literal = username = '<sysadmin_username>' --from-literal = password = '<sysadmin_password>' --from-literal = displayName = '<sysadmin_display_name>' --from-literal = emailAddress = '<sysadmin_email>' Update the values.yaml file with the secrets: license : secretName : <secret_name> secretKey : license-key ... sysadminCredentials : secretName : <sysadmin_creds_secret_name> usernameSecretKey : username passwordSecretKey : password displayNameSecretKey : displayName emailAddressSecretKey : emailAddress 7. Install your chosen product \u00b6 helm install <release-name> \\ atlassian-data-center/<product> \\ --namespace <namespace> \\ --version <chart-version> \\ --values values.yaml <release-name> is the name of your deployment and is up to you, or you can use --generate-name . <product> can be jira, confluence, bitbucket or crowd. <namespace> is optional. You can use namespaces to organize clusters into virtual sub-clusters. <chart-version> is optional, and can be omitted if you just want to use the latest version of the chart. values.yaml is optional and contains your site-specific configuration information. If omitted, the chart config default will be used. Add --wait if you wish the installation command to block until all of the deployed Kubernetes resources are ready, but be aware that this may be waiting for several minutes if anything is mis-configured. 8. Test your deployed product \u00b6 Make sure the service pod/s are running, then test your deployed product: helm test <release-name> --logs --namespace <namespace> This will run some basic smoke tests against the deployed release. If any of these tests fail, it is likely that the deployment was not successful. Please check the status of the deployed resources for any obvious errors that may have caused the failure. 9. Complete product setup \u00b6 Using the service URL provided by Helm post install, open your product in a web browser and complete the setup via the setup wizard. Uninstall \u00b6 helm uninstall <release-name> atlassian-data-center/<product> <release-name> is the name you chose for your deployment <product> can be jira, confluence, bitbucket, or crowd","title":"Installation"},{"location":"installation/INSTALLATION/#installation","text":"Follow these instructions to install your Atlassian product using the Helm charts. Before you proceed with the installation, make sure you have followed the Prerequisites guide .","title":"Installation"},{"location":"installation/INSTALLATION/#1-add-the-helm-chart-repository","text":"Add the Helm chart repository to your local Helm installation: helm repo add atlassian-data-center \\ https://atlassian-labs.github.io/data-center-helm-charts Update the repo: helm repo update","title":"1. Add the Helm chart repository"},{"location":"installation/INSTALLATION/#2-obtain-valuesyaml","text":"Obtain the default product values.yaml file from the chart: helm show values atlassian-data-center/<product> > values.yaml","title":"2. Obtain values.yaml"},{"location":"installation/INSTALLATION/#3-configure-database","text":"Using the values.yaml file obtained in step 2 , configure the usage of the database provisioned as part of the prerequisites . By providing all the required database values, you will bypass the database connectivity configuration during the product setup. Migration If you are migrating an existing Data Center product to Kubernetes, use the values of your product's database. See Migration guide . Create a Kubernetes secret to store the connectivity details of the database: kubectl create secret generic <secret_name> --from-literal = username = '<db_username>' --from-literal = password = '<db_password>' Using the Kubernetes secret, update the database stanza within values.yaml appropriately. Refer to the commentary within the values.yaml file for additional details on how to configure the remaining database values: database : type : <db_type> url : <jdbc_url> driver : <engine_driver> credentials : secretName : <secret_name> usernameSecretKey : username passwordSecretKey : password For additional information on how the above values should be configured, see the Database connectivity section of the configuration guide . Read about Kubernetes secrets .","title":"3. Configure database"},{"location":"installation/INSTALLATION/#4-configure-ingress","text":"Using the values.yaml file obtained in step 2 , configure the Ingress controller provisioned as part of the Prerequisites . The values you provide here will be used to provision an Ingress resource for the controller. Refer to the associated comments within the values.yaml file for additional details on how to configure the Ingress resource: ingress : create : true #1. Setting true here will create an Ingress resource nginx : true #2. If using the ingress-nginx controller set this property to true maxBodySize : 250m host : <dns_host_name> #2. Hosts can be precise matches (for example \u201cfoo.bar.com\u201d) or a wildcard (for example \u201c*.foo.com\u201d). path : \"/\" annotations : cert-manager.io/issuer : <certificate_issuer> https : true tlsSecretName : <tls_certificate_name> For additional details on Ingress controllers see the Ingress section of the configuration guide . See an example of how to set up a controller .","title":"4. Configure Ingress"},{"location":"installation/INSTALLATION/#5-configure-persistent-storage","text":"Using the values.yaml file obtained in step 2 , configure the shared-home that was provisioned as part of the Prerequisites . See shared home example . If you are migrating an existing Data Center product to Kubernetes , use the values of your product's shared home. volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : <pvc_name> Each pod will also require its own local-home storage. This can be configured with a StorageClass , as can be seen in the local home example . Having created the StorageClass , update values.yaml to make use of it: volumes : localHome : persistentVolumeClaim : create : true storageClassName : <storage-class-name> For more details, please refer to the Volumes section of the configuration guide . Bitbucket shared storage Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket .","title":"5. Configure persistent storage"},{"location":"installation/INSTALLATION/#6-configure-license-and-sysadmin-credentials-for-bitbucket","text":"Bitbucket is slightly different from the other products in that it can be completely configured during deployment, meaning no manual setup is required. To do this, you need to update the sysadminCredentials and license stanzas within the values.yaml obtained in step 2 . Create a Kubernetes secret to hold the Bitbucket license: kubectl create secret generic <license_secret_name> --from-literal = license-key = '<bitbucket_license_key>' Create a Kubernetes secret to hold the Bitbucket system administrator credentials: kubectl create secret generic <sysadmin_creds_secret_name> --from-literal = username = '<sysadmin_username>' --from-literal = password = '<sysadmin_password>' --from-literal = displayName = '<sysadmin_display_name>' --from-literal = emailAddress = '<sysadmin_email>' Update the values.yaml file with the secrets: license : secretName : <secret_name> secretKey : license-key ... sysadminCredentials : secretName : <sysadmin_creds_secret_name> usernameSecretKey : username passwordSecretKey : password displayNameSecretKey : displayName emailAddressSecretKey : emailAddress","title":"6. Configure license and sysadmin credentials for Bitbucket"},{"location":"installation/INSTALLATION/#7-install-your-chosen-product","text":"helm install <release-name> \\ atlassian-data-center/<product> \\ --namespace <namespace> \\ --version <chart-version> \\ --values values.yaml <release-name> is the name of your deployment and is up to you, or you can use --generate-name . <product> can be jira, confluence, bitbucket or crowd. <namespace> is optional. You can use namespaces to organize clusters into virtual sub-clusters. <chart-version> is optional, and can be omitted if you just want to use the latest version of the chart. values.yaml is optional and contains your site-specific configuration information. If omitted, the chart config default will be used. Add --wait if you wish the installation command to block until all of the deployed Kubernetes resources are ready, but be aware that this may be waiting for several minutes if anything is mis-configured.","title":"7. Install your chosen product"},{"location":"installation/INSTALLATION/#8-test-your-deployed-product","text":"Make sure the service pod/s are running, then test your deployed product: helm test <release-name> --logs --namespace <namespace> This will run some basic smoke tests against the deployed release. If any of these tests fail, it is likely that the deployment was not successful. Please check the status of the deployed resources for any obvious errors that may have caused the failure.","title":"8. Test your deployed product"},{"location":"installation/INSTALLATION/#9-complete-product-setup","text":"Using the service URL provided by Helm post install, open your product in a web browser and complete the setup via the setup wizard.","title":"9. Complete product setup"},{"location":"installation/INSTALLATION/#uninstall","text":"helm uninstall <release-name> atlassian-data-center/<product> <release-name> is the name you chose for your deployment <product> can be jira, confluence, bitbucket, or crowd","title":"Uninstall"},{"location":"installation/MIGRATION/","text":"Migration \u00b6 If you already have an existing Data Center product deployment, you can migrate it to a Kubernetes cluster using the Data Center Helm charts. You will need to migrate your database and your shared home, then all you need to do is to follow the Installation guide , using your migrated resources instead of provisioning new ones. Migrating your database - To migrate your database, you should point the Helm charts to the existing database or to a migrated version of the database. Do this by updating the database stanza in the values.yaml file as explained in the Configure database step in the installation guide . Migrating your shared home - Application nodes should have access to a shared directory in the same path. Examples of what the shared file system stores include plugins, shared caches, repositories, attachments, and avatars. Configure your shared home by updating the sharedHome stanza in the values.yaml file as explained in the Configure persistent storage step in the installation guide . Helpful links: Helm charts Atlassian Data Center migration plan - gives some guidance on overall process, organizational preparedness, estimated time frames, and app compatibility. Atlassian Data Center migration checklist - also provides useful tests and checks to perform throughout the moving process. Migrating to another database - describes how to migrate your data from your existing database to another database: Migrating Confluence to another database Migrating Jira to another database For better performance consider co-locating your migrated database in the Availability Zone (AZ) as your product nodes. Database-heavy operations (e.g. full re-index) become significantly faster when the database is collocated with the Data Center node in the same AZ, however we don't recommend this if you're running critical workloads.","title":"Migration"},{"location":"installation/MIGRATION/#migration","text":"If you already have an existing Data Center product deployment, you can migrate it to a Kubernetes cluster using the Data Center Helm charts. You will need to migrate your database and your shared home, then all you need to do is to follow the Installation guide , using your migrated resources instead of provisioning new ones. Migrating your database - To migrate your database, you should point the Helm charts to the existing database or to a migrated version of the database. Do this by updating the database stanza in the values.yaml file as explained in the Configure database step in the installation guide . Migrating your shared home - Application nodes should have access to a shared directory in the same path. Examples of what the shared file system stores include plugins, shared caches, repositories, attachments, and avatars. Configure your shared home by updating the sharedHome stanza in the values.yaml file as explained in the Configure persistent storage step in the installation guide . Helpful links: Helm charts Atlassian Data Center migration plan - gives some guidance on overall process, organizational preparedness, estimated time frames, and app compatibility. Atlassian Data Center migration checklist - also provides useful tests and checks to perform throughout the moving process. Migrating to another database - describes how to migrate your data from your existing database to another database: Migrating Confluence to another database Migrating Jira to another database For better performance consider co-locating your migrated database in the Availability Zone (AZ) as your product nodes. Database-heavy operations (e.g. full re-index) become significantly faster when the database is collocated with the Data Center node in the same AZ, however we don't recommend this if you're running critical workloads.","title":"Migration"},{"location":"installation/PREREQUISITES/","text":"Prerequisites \u00b6 Requirements \u00b6 In order to deploy Atlassian\u2019s Data Center products, the following is required: An understanding of Kubernetes and Helm concepts A Kubernetes cluster, running Kubernetes v1.19 or later kubectl v1.19 or later, must be compatible with your cluster helm v3.3 or later Environment setup \u00b6 Before installing the Data Center Helm charts you need to set up your environment: Install tools \u00b6 Install Helm Install kubectl Create and connect to the Kubernetes cluster \u00b6 See examples of provisioning Kubernetes clusters on cloud-based providers . In order to install the charts to your Kubernetes cluster, your Kubernetes client config must be configured appropriately, and you must have the necessary permissions. It is up to you to set up security policies. Provision an Ingress Controller \u00b6 See an example of provisioning an NGINX Ingress Controller . This step is necessary in order to make your Atlassian product available from outside of the Kubernetes cluster after deployment. The Kubernetes project supports and maintains ingress controllers for the major cloud providers including; AWS , GCE and nginx . There are also a number of open-source third-party projects available . Because different Kubernetes clusters use different ingress configurations/controllers, the Helm charts provide Ingress Object templates only. The Ingress resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml (an alternative controller can be used). For more information about the Ingress controller go to the Ingress section of the configuration guide . Provision a database \u00b6 See an example of provisioning databases on cloud-based providers . Must be of a type and version supported by the Data Center product you wish to install: Jira Supported databases Confluence Supported databases Bitbucket Supported databases Crowd Supported databases Must be reachable from the product deployed within your Kubernetes cluster. The database service may be deployed within the same Kubernetes cluster as the Data Center product or elsewhere. The products need to be provided with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. For more information go to the Database connectivity section of the configuration guide . For better performance consider co-locating your database in the Availability Zone (AZ) as your product nodes. Database-heavy operations (e.g. full re-index) become significantly faster when the database is collocated with the Data Center node in the same AZ, however we don't recommend this if you're running critical workloads. Configure a shared-home volume \u00b6 See examples of creating shared storage . All of the Data Center products require a shared network filesystem if they are to be operated in multi-node clusters. If no shared filesystem is available, the products can only be operated in single-node configuration. The shared-home volume must be correctly configured as a read-write shared filesystem (e.g. NFS, AWS EFS, Azure Files) The recommended setup is to use Kubernetes PersistentVolumes and PersistentVolumeClaims. The local-home volume requires a PersistentVolume with ReadWriteOnce (RWO) capability, and shared-home requires a PersistentVolume with ReadWriteMany (RWX) capability. Typically, this will be a NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own RWX volumes (e.g. AzureFile, ElasticFileStore). For more information about volumes go to the Volumes section of the configuration guide .","title":"Prerequisites"},{"location":"installation/PREREQUISITES/#prerequisites","text":"","title":"Prerequisites"},{"location":"installation/PREREQUISITES/#requirements","text":"In order to deploy Atlassian\u2019s Data Center products, the following is required: An understanding of Kubernetes and Helm concepts A Kubernetes cluster, running Kubernetes v1.19 or later kubectl v1.19 or later, must be compatible with your cluster helm v3.3 or later","title":"Requirements"},{"location":"installation/PREREQUISITES/#environment-setup","text":"Before installing the Data Center Helm charts you need to set up your environment:","title":"Environment setup"},{"location":"installation/PREREQUISITES/#install-tools","text":"Install Helm Install kubectl","title":"Install tools"},{"location":"installation/PREREQUISITES/#create-and-connect-to-the-kubernetes-cluster","text":"See examples of provisioning Kubernetes clusters on cloud-based providers . In order to install the charts to your Kubernetes cluster, your Kubernetes client config must be configured appropriately, and you must have the necessary permissions. It is up to you to set up security policies.","title":"Create and connect to the Kubernetes cluster"},{"location":"installation/PREREQUISITES/#provision-an-ingress-controller","text":"See an example of provisioning an NGINX Ingress Controller . This step is necessary in order to make your Atlassian product available from outside of the Kubernetes cluster after deployment. The Kubernetes project supports and maintains ingress controllers for the major cloud providers including; AWS , GCE and nginx . There are also a number of open-source third-party projects available . Because different Kubernetes clusters use different ingress configurations/controllers, the Helm charts provide Ingress Object templates only. The Ingress resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml (an alternative controller can be used). For more information about the Ingress controller go to the Ingress section of the configuration guide .","title":"Provision an Ingress Controller"},{"location":"installation/PREREQUISITES/#provision-a-database","text":"See an example of provisioning databases on cloud-based providers . Must be of a type and version supported by the Data Center product you wish to install: Jira Supported databases Confluence Supported databases Bitbucket Supported databases Crowd Supported databases Must be reachable from the product deployed within your Kubernetes cluster. The database service may be deployed within the same Kubernetes cluster as the Data Center product or elsewhere. The products need to be provided with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. For more information go to the Database connectivity section of the configuration guide . For better performance consider co-locating your database in the Availability Zone (AZ) as your product nodes. Database-heavy operations (e.g. full re-index) become significantly faster when the database is collocated with the Data Center node in the same AZ, however we don't recommend this if you're running critical workloads.","title":"Provision a database"},{"location":"installation/PREREQUISITES/#configure-a-shared-home-volume","text":"See examples of creating shared storage . All of the Data Center products require a shared network filesystem if they are to be operated in multi-node clusters. If no shared filesystem is available, the products can only be operated in single-node configuration. The shared-home volume must be correctly configured as a read-write shared filesystem (e.g. NFS, AWS EFS, Azure Files) The recommended setup is to use Kubernetes PersistentVolumes and PersistentVolumeClaims. The local-home volume requires a PersistentVolume with ReadWriteOnce (RWO) capability, and shared-home requires a PersistentVolume with ReadWriteMany (RWX) capability. Typically, this will be a NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own RWX volumes (e.g. AzureFile, ElasticFileStore). For more information about volumes go to the Volumes section of the configuration guide .","title":"Configure a shared-home volume"},{"location":"operations/OPERATION/","text":"Operation \u00b6 Once you have installed your product , use this document if you want to scale your product, update your product, or see what examples we have. Managing resources \u00b6 You can scale your application by adding additonal pods or by managing available resources with requests and limits . Upgrading application \u00b6 Kubernetes update strategies \u00b6 Kubernetes provides two strategies to update applications managed by statefulset controllers: Rolling update \u00b6 The pods will be upgraded one by one until all pods run containers with the updated template. The upgrade is managed by Kubernetes and the user has limited control during the upgrade process, after having modified the template. This is the default upgrade strategy in Kubernetes. To perform a canary or multi-phase upgrade, a partition can be defined on the cluster and Kubernetes will upgrade just the nodes in that partition. The default implementation is based on RollingUpdate strategy with no partition defined. OnDelete strategy \u00b6 In this strategy users select the pod to upgrade by deleting it, and Kubernetes will replace it by creating a new pod based on the updated template. To select this strategy the following should be replaced with the current implementation of updateStrategy in the statefulset spec: updateStrategy : type : OnDelete Rolling upgrade \u00b6 Jira To learn about rolling upgrade in Jira see Jira rolling upgrade Confluence To learn about rolling upgrade in Confluence see Confluence rolling upgrade Bitbucket To learn about rolling upgrade in Bitbucket see Bitbucket rolling upgrade Examples \u00b6 Logging \u00b6 How to deploy an EFK stack to Kubernetes \u00b6 There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster. Please refer to Logging in Kubernetes . Storage \u00b6 Example implementation of NFS Server for Bitbucket \u00b6 Bitbucket Data Center (Bitbucket DC) uses a shared network file system (NFS) to store its repositories in a common location that is accessible to multiple Bitbucket nodes. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but we provide an example implementation of NFS Server for Bitbucket , which can be used as a guide. Examples of provisioning storage with the AWS CSI Driver \u00b6 Local storage - utilizing AWS EBS-backed volumes Shared storage - utilizing AWS EFS-backed filesystem","title":"Operation"},{"location":"operations/OPERATION/#operation","text":"Once you have installed your product , use this document if you want to scale your product, update your product, or see what examples we have.","title":"Operation"},{"location":"operations/OPERATION/#managing-resources","text":"You can scale your application by adding additonal pods or by managing available resources with requests and limits .","title":"Managing resources"},{"location":"operations/OPERATION/#upgrading-application","text":"","title":"Upgrading application"},{"location":"operations/OPERATION/#kubernetes-update-strategies","text":"Kubernetes provides two strategies to update applications managed by statefulset controllers:","title":"Kubernetes update strategies"},{"location":"operations/OPERATION/#rolling-update","text":"The pods will be upgraded one by one until all pods run containers with the updated template. The upgrade is managed by Kubernetes and the user has limited control during the upgrade process, after having modified the template. This is the default upgrade strategy in Kubernetes. To perform a canary or multi-phase upgrade, a partition can be defined on the cluster and Kubernetes will upgrade just the nodes in that partition. The default implementation is based on RollingUpdate strategy with no partition defined.","title":"Rolling update"},{"location":"operations/OPERATION/#ondelete-strategy","text":"In this strategy users select the pod to upgrade by deleting it, and Kubernetes will replace it by creating a new pod based on the updated template. To select this strategy the following should be replaced with the current implementation of updateStrategy in the statefulset spec: updateStrategy : type : OnDelete","title":"OnDelete strategy"},{"location":"operations/OPERATION/#rolling-upgrade","text":"Jira To learn about rolling upgrade in Jira see Jira rolling upgrade Confluence To learn about rolling upgrade in Confluence see Confluence rolling upgrade Bitbucket To learn about rolling upgrade in Bitbucket see Bitbucket rolling upgrade","title":"Rolling upgrade"},{"location":"operations/OPERATION/#examples","text":"","title":"Examples"},{"location":"operations/OPERATION/#logging","text":"","title":"Logging"},{"location":"operations/OPERATION/#how-to-deploy-an-efk-stack-to-kubernetes","text":"There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster. Please refer to Logging in Kubernetes .","title":"How to deploy an EFK stack to Kubernetes"},{"location":"operations/OPERATION/#storage","text":"","title":"Storage"},{"location":"operations/OPERATION/#example-implementation-of-nfs-server-for-bitbucket","text":"Bitbucket Data Center (Bitbucket DC) uses a shared network file system (NFS) to store its repositories in a common location that is accessible to multiple Bitbucket nodes. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but we provide an example implementation of NFS Server for Bitbucket , which can be used as a guide.","title":"Example implementation of NFS Server for Bitbucket"},{"location":"operations/OPERATION/#examples-of-provisioning-storage-with-the-aws-csi-driver","text":"Local storage - utilizing AWS EBS-backed volumes Shared storage - utilizing AWS EFS-backed filesystem","title":"Examples of provisioning storage with the AWS CSI Driver"},{"location":"operations/product_upgrades/BITBUCKET_UPGRADE/","text":"Bitbucket rolling upgrade \u00b6 Let's say we have Bitbucket version 7.6.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.6.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. 1. Find tag of the target image \u00b6 Go to atlassian/bitbucket-server Docker Hub page to pick a tag that matches your target version. In the example we're running Bitbucket using the 7.6.0-jdk11 tag, and we'll be upgrading to 7.6.1-jdk11 - our target . 2. Put Bitbucket into upgrade mode \u00b6 From the admin page click on Rolling Upgrade and set the Bitbucket to Upgrade mode: 3. Run the upgrade using Helm \u00b6 Run Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . $ helm upgrade <release-name> atlassian-data-center/bitbucket --wait --reuse-values --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-bb-nodes> to the command. Otherwise, the deployment will be scaled back to the original number which, most likely, is one node. 4. Wait for the upgrade to finish \u00b6 The pods will be re-created with the updated version, one at a time. 5. Finalize the upgrade \u00b6 After all pods are active with the new version, finalize the upgrade:","title":"Bitbucket"},{"location":"operations/product_upgrades/BITBUCKET_UPGRADE/#bitbucket-rolling-upgrade","text":"Let's say we have Bitbucket version 7.6.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.6.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one.","title":"Bitbucket rolling upgrade"},{"location":"operations/product_upgrades/BITBUCKET_UPGRADE/#1-find-tag-of-the-target-image","text":"Go to atlassian/bitbucket-server Docker Hub page to pick a tag that matches your target version. In the example we're running Bitbucket using the 7.6.0-jdk11 tag, and we'll be upgrading to 7.6.1-jdk11 - our target .","title":"1. Find tag of the target image"},{"location":"operations/product_upgrades/BITBUCKET_UPGRADE/#2-put-bitbucket-into-upgrade-mode","text":"From the admin page click on Rolling Upgrade and set the Bitbucket to Upgrade mode:","title":"2. Put Bitbucket into upgrade mode"},{"location":"operations/product_upgrades/BITBUCKET_UPGRADE/#3-run-the-upgrade-using-helm","text":"Run Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . $ helm upgrade <release-name> atlassian-data-center/bitbucket --wait --reuse-values --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-bb-nodes> to the command. Otherwise, the deployment will be scaled back to the original number which, most likely, is one node.","title":"3. Run the upgrade using Helm"},{"location":"operations/product_upgrades/BITBUCKET_UPGRADE/#4-wait-for-the-upgrade-to-finish","text":"The pods will be re-created with the updated version, one at a time.","title":"4. Wait for the upgrade to finish"},{"location":"operations/product_upgrades/BITBUCKET_UPGRADE/#5-finalize-the-upgrade","text":"After all pods are active with the new version, finalize the upgrade:","title":"5. Finalize the upgrade"},{"location":"operations/product_upgrades/CONFLUENCE_UPGRADE/","text":"Confluence rolling upgrade \u00b6 Let's say we have Confluence version 7.4.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.4.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. 1. Find tag of the target image \u00b6 Go to atlassian/confluence-server Docker Hub page to pick a tag that matches your target version. In the example we're running Confluence using the 7.4.0-jdk11 tag, and we'll be upgrading to 7.4.1-jdk11 - our target . 2. Put Confluence into upgrade mode \u00b6 From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode: 3. Run the upgrade using Helm \u00b6 Run Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . $ helm upgrade <release-name> atlassian-data-center/confluence --wait --reuse-values --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-confluence-nodes> to the command. Otherwise, the deployment will be scaled back to the original number which, most likely, is one node. 4. Wait for the upgrade to finish \u00b6 The pods will be re-created with the updated version, one at a time. 5. Finalize the upgrade \u00b6 After all pods activated with the new version, finalize the upgrade:","title":"Confluence"},{"location":"operations/product_upgrades/CONFLUENCE_UPGRADE/#confluence-rolling-upgrade","text":"Let's say we have Confluence version 7.4.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.4.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one.","title":"Confluence rolling upgrade"},{"location":"operations/product_upgrades/CONFLUENCE_UPGRADE/#1-find-tag-of-the-target-image","text":"Go to atlassian/confluence-server Docker Hub page to pick a tag that matches your target version. In the example we're running Confluence using the 7.4.0-jdk11 tag, and we'll be upgrading to 7.4.1-jdk11 - our target .","title":"1. Find tag of the target image"},{"location":"operations/product_upgrades/CONFLUENCE_UPGRADE/#2-put-confluence-into-upgrade-mode","text":"From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode:","title":"2. Put Confluence into upgrade mode"},{"location":"operations/product_upgrades/CONFLUENCE_UPGRADE/#3-run-the-upgrade-using-helm","text":"Run Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . $ helm upgrade <release-name> atlassian-data-center/confluence --wait --reuse-values --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-confluence-nodes> to the command. Otherwise, the deployment will be scaled back to the original number which, most likely, is one node.","title":"3. Run the upgrade using Helm"},{"location":"operations/product_upgrades/CONFLUENCE_UPGRADE/#4-wait-for-the-upgrade-to-finish","text":"The pods will be re-created with the updated version, one at a time.","title":"4. Wait for the upgrade to finish"},{"location":"operations/product_upgrades/CONFLUENCE_UPGRADE/#5-finalize-the-upgrade","text":"After all pods activated with the new version, finalize the upgrade:","title":"5. Finalize the upgrade"},{"location":"operations/product_upgrades/JIRA_UPGRADE/","text":"Jira rolling upgrade \u00b6 Let's say we have Jira version 8.13.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 8.13.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. 1. Find tag of the target image \u00b6 Go to atlassian/jira-software Docker Hub page to pick a tag that matches your target version. In the example we're running Jira using the 8.13.0-jdk11 tag, and we'll be upgrading to 8.13.1-jdk11 - our target . 2. Put Jira into upgrade mode \u00b6 Go to Administration > Applications > Jira upgrades and click Put Jira into upgrade mode . 3. Run the upgrade using Helm \u00b6 Run Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . $ helm upgrade <release-name> atlassian-data-center/jira --wait --reuse-values --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-jira-nodes> to the command. Otherwise, the deployment will be scaled back to the original number which, most likely, is one node. 4. Wait for the upgrade to finish \u00b6 The pods will be re-created with the updated version, one at a time. 5. Finalize the upgrade \u00b6 After all pods are active with the new version, click Run upgrade tasks to finalize the upgrade:","title":"Jira"},{"location":"operations/product_upgrades/JIRA_UPGRADE/#jira-rolling-upgrade","text":"Let's say we have Jira version 8.13.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 8.13.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one.","title":"Jira rolling upgrade"},{"location":"operations/product_upgrades/JIRA_UPGRADE/#1-find-tag-of-the-target-image","text":"Go to atlassian/jira-software Docker Hub page to pick a tag that matches your target version. In the example we're running Jira using the 8.13.0-jdk11 tag, and we'll be upgrading to 8.13.1-jdk11 - our target .","title":"1. Find tag of the target image"},{"location":"operations/product_upgrades/JIRA_UPGRADE/#2-put-jira-into-upgrade-mode","text":"Go to Administration > Applications > Jira upgrades and click Put Jira into upgrade mode .","title":"2. Put Jira into upgrade mode"},{"location":"operations/product_upgrades/JIRA_UPGRADE/#3-run-the-upgrade-using-helm","text":"Run Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . $ helm upgrade <release-name> atlassian-data-center/jira --wait --reuse-values --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-jira-nodes> to the command. Otherwise, the deployment will be scaled back to the original number which, most likely, is one node.","title":"3. Run the upgrade using Helm"},{"location":"operations/product_upgrades/JIRA_UPGRADE/#4-wait-for-the-upgrade-to-finish","text":"The pods will be re-created with the updated version, one at a time.","title":"4. Wait for the upgrade to finish"},{"location":"operations/product_upgrades/JIRA_UPGRADE/#5-finalize-the-upgrade","text":"After all pods are active with the new version, click Run upgrade tasks to finalize the upgrade:","title":"5. Finalize the upgrade"},{"location":"operations/resource_management/REQUESTS_AND_LIMITS/","text":"Resource requests and limits \u00b6 To ensure that Kubernetes appropriately schedules resources, the respective product values.yaml is configured with default cpu and memory resource request values . Resource requests \u00b6 The default resource requests that are used for each product are defined below. Take note that these values are geared toward small data sets. For larger enterprise deployments refer to the data center infrastructure recommendations here . Using the formula below, the memory specific values are derived from the default JVM requirements defined for each product's Docker container. Product CPU Memory Jira 2 2G Confluence 2 2G Bitbucket 2 2G Crowd 2 1G Memory request sizing \u00b6 Request sizing must allow for the size of the product JVM . That means the maximum heap size , minumum heap size and the reserved code cache size (if applicable) plus other JVM overheads, must be considered when defining the request memory size. As a rule of thumb the formula below can be used to deduce the appropriate request memory size. ( maxHeap + codeCache ) * 1 .5 Resource limits \u00b6 Environmental and hardware constraints are different for each deployment, therefore the product values.yaml do not provide a resource limit definition. Resource usage limits can be defined by updating the commented out resources.container.limits stanza within the appropriate product values.yaml , for example: container : limits : cpu : \"2\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\"","title":"Resource requests and limits"},{"location":"operations/resource_management/REQUESTS_AND_LIMITS/#resource-requests-and-limits","text":"To ensure that Kubernetes appropriately schedules resources, the respective product values.yaml is configured with default cpu and memory resource request values .","title":"Resource requests and limits"},{"location":"operations/resource_management/REQUESTS_AND_LIMITS/#resource-requests","text":"The default resource requests that are used for each product are defined below. Take note that these values are geared toward small data sets. For larger enterprise deployments refer to the data center infrastructure recommendations here . Using the formula below, the memory specific values are derived from the default JVM requirements defined for each product's Docker container. Product CPU Memory Jira 2 2G Confluence 2 2G Bitbucket 2 2G Crowd 2 1G","title":"Resource requests"},{"location":"operations/resource_management/REQUESTS_AND_LIMITS/#memory-request-sizing","text":"Request sizing must allow for the size of the product JVM . That means the maximum heap size , minumum heap size and the reserved code cache size (if applicable) plus other JVM overheads, must be considered when defining the request memory size. As a rule of thumb the formula below can be used to deduce the appropriate request memory size. ( maxHeap + codeCache ) * 1 .5","title":"Memory request sizing"},{"location":"operations/resource_management/REQUESTS_AND_LIMITS/#resource-limits","text":"Environmental and hardware constraints are different for each deployment, therefore the product values.yaml do not provide a resource limit definition. Resource usage limits can be defined by updating the commented out resources.container.limits stanza within the appropriate product values.yaml , for example: container : limits : cpu : \"2\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\"","title":"Resource limits"},{"location":"operations/resource_management/RESOURCE_SCALING/","text":"Product scaling \u00b6 For optimum performance and stability the appropriate resource requests and limits should be defined for each pod. The number of pods in the product cluster should also be carefully considered. Kubernetes provides means for horizontal and vertical scaling of the deployed pods within a cluster, these approaches are described below. Horizontal scaling - adding pods \u00b6 The Helm charts provision one StatefulSet by default. The number of replicas within this StatefulSet can be altered either declaratively or imperatively. Note that the Ingress must support cookie-based session affinity in order for the products to work correctly in a multi-node configuration. Declaratively Update values.yaml by modifying the replicaCount appropriately. Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively kubectl scale statefulsets <statefulsetset-name> --replicas = n Initial cluster size Jira , Confluence , and Crowd all require manual configuration after the first pod is deployed and before scaling up to additional pods, therefore when you deploy the product only one pod (replica) is created. The initial number of pods that should be started at deployment of each product is set in the replicaCount variable found in the values.yaml and should always be kept as 1. For details on modifying the cpu and memory requirements of the StatfuleSet see section Vertical Scaling below. Additional details on the resource requests and limits used by the StatfulSet can be found in Resource requests and limits page. Vertical scaling - adding resources \u00b6 The resource requests and limits for a StatefulSet can be defined before product deployment or for deployments that are already running within the Kubernetes cluster. Take note that vertical scaling will result in the pod being re-created with the updated values. Prior to deployment \u00b6 Before performing a helm install update the appropriate products values.yaml container stanza with the desired requests and limits values i.e. container : limits : cpu : \"4\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\" Post deployment \u00b6 For existing deployments the requests and limits values can be dynamically updated either declaratively or imperatively Declaratively This the preferred approach as it keeps the state of the cluster, and the helm charts themselves in sync. Update values.yaml appropriately Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively Using kubectl edit on the appropriate StatefulSet the respective cpu and memory values can be modified. Saving the changes will then result in the existing product pod(s) being re-provisioned with the updated values.","title":"Product scaling"},{"location":"operations/resource_management/RESOURCE_SCALING/#product-scaling","text":"For optimum performance and stability the appropriate resource requests and limits should be defined for each pod. The number of pods in the product cluster should also be carefully considered. Kubernetes provides means for horizontal and vertical scaling of the deployed pods within a cluster, these approaches are described below.","title":"Product scaling"},{"location":"operations/resource_management/RESOURCE_SCALING/#horizontal-scaling-adding-pods","text":"The Helm charts provision one StatefulSet by default. The number of replicas within this StatefulSet can be altered either declaratively or imperatively. Note that the Ingress must support cookie-based session affinity in order for the products to work correctly in a multi-node configuration. Declaratively Update values.yaml by modifying the replicaCount appropriately. Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively kubectl scale statefulsets <statefulsetset-name> --replicas = n Initial cluster size Jira , Confluence , and Crowd all require manual configuration after the first pod is deployed and before scaling up to additional pods, therefore when you deploy the product only one pod (replica) is created. The initial number of pods that should be started at deployment of each product is set in the replicaCount variable found in the values.yaml and should always be kept as 1. For details on modifying the cpu and memory requirements of the StatfuleSet see section Vertical Scaling below. Additional details on the resource requests and limits used by the StatfulSet can be found in Resource requests and limits page.","title":"Horizontal scaling - adding pods"},{"location":"operations/resource_management/RESOURCE_SCALING/#vertical-scaling-adding-resources","text":"The resource requests and limits for a StatefulSet can be defined before product deployment or for deployments that are already running within the Kubernetes cluster. Take note that vertical scaling will result in the pod being re-created with the updated values.","title":"Vertical scaling - adding resources"},{"location":"operations/resource_management/RESOURCE_SCALING/#prior-to-deployment","text":"Before performing a helm install update the appropriate products values.yaml container stanza with the desired requests and limits values i.e. container : limits : cpu : \"4\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\"","title":"Prior to deployment"},{"location":"operations/resource_management/RESOURCE_SCALING/#post-deployment","text":"For existing deployments the requests and limits values can be dynamically updated either declaratively or imperatively Declaratively This the preferred approach as it keeps the state of the cluster, and the helm charts themselves in sync. Update values.yaml appropriately Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively Using kubectl edit on the appropriate StatefulSet the respective cpu and memory values can be modified. Saving the changes will then result in the existing product pod(s) being re-provisioned with the updated values.","title":"Post deployment"},{"location":"platforms/OPENSHIFT/","text":"OpenShift \u00b6 The Helm charts are vendor agnostic and create objects from standard APIs that OpenShift fully supports. However, by default OpenShift will not allow running containers as users specified in the image Dockerfiles or securityContext.fsGroup in a statefulset/deployment spec. There are a couple of ways to fix it. Attach anyuid policies \u00b6 If possible, attach anyuid policy to 2 serviceAccounts. Here's an example for a Bitbucket installation. Please, note that the service account names vary depending on the Data Center product: For Bitbucket pods oc adm policy add-scc-to-user anyuid -z bitbucket -n git For NFS permission fixer pod oc adm policy add-scc-to-user anyuid -z default -n git Typically, volumes.sharedHome.persistentVolumeClaim.nfsPermissionFixer needs to be set to true to make volume writable. It depends on the storage backend though. Set no security context \u00b6 As an alternative, (if letting containers run as pre-defined users is not possible), set product_name.securityContext.enabled to false . As a result the container will start as a user with an OpenShift generated ID. Typically, NFS permission fixer job isn't required when no security context is set. OpenShift Routes \u00b6 The Helm charts do not have templates for OpenShift routes that are commonly used in OpenShift instead of ingresses. Routes need to be manually created after the charts installation.","title":"OpenShift"},{"location":"platforms/OPENSHIFT/#openshift","text":"The Helm charts are vendor agnostic and create objects from standard APIs that OpenShift fully supports. However, by default OpenShift will not allow running containers as users specified in the image Dockerfiles or securityContext.fsGroup in a statefulset/deployment spec. There are a couple of ways to fix it.","title":"OpenShift"},{"location":"platforms/OPENSHIFT/#attach-anyuid-policies","text":"If possible, attach anyuid policy to 2 serviceAccounts. Here's an example for a Bitbucket installation. Please, note that the service account names vary depending on the Data Center product: For Bitbucket pods oc adm policy add-scc-to-user anyuid -z bitbucket -n git For NFS permission fixer pod oc adm policy add-scc-to-user anyuid -z default -n git Typically, volumes.sharedHome.persistentVolumeClaim.nfsPermissionFixer needs to be set to true to make volume writable. It depends on the storage backend though.","title":"Attach anyuid policies"},{"location":"platforms/OPENSHIFT/#set-no-security-context","text":"As an alternative, (if letting containers run as pre-defined users is not possible), set product_name.securityContext.enabled to false . As a result the container will start as a user with an OpenShift generated ID. Typically, NFS permission fixer job isn't required when no security context is set.","title":"Set no security context"},{"location":"platforms/OPENSHIFT/#openshift-routes","text":"The Helm charts do not have templates for OpenShift routes that are commonly used in OpenShift instead of ingresses. Routes need to be manually created after the charts installation.","title":"OpenShift Routes"},{"location":"platforms/PLATFORMS/","text":"Platform details \u00b6 Support disclaimer The platforms documented on this page are not officially supported as part of the Atlassian Data Center products, and should be used only as examples. Platforms \u00b6 OpenShift","title":"Platform details"},{"location":"platforms/PLATFORMS/#platform-details","text":"Support disclaimer The platforms documented on this page are not officially supported as part of the Atlassian Data Center products, and should be used only as examples.","title":"Platform details"},{"location":"platforms/PLATFORMS/#platforms","text":"OpenShift","title":"Platforms"}]}